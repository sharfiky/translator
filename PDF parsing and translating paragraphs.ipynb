{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:39.029446Z",
     "start_time": "2020-12-19T15:06:38.407858Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdfminer.high_level\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:39.045439Z",
     "start_time": "2020-12-19T15:06:39.033426Z"
    }
   },
   "outputs": [],
   "source": [
    "# pdf_name = \"example_article.pdf\"\n",
    "pdf_name = \"cycle_gan.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.586366Z",
     "start_time": "2020-12-19T15:06:39.049413Z"
    }
   },
   "outputs": [],
   "source": [
    "text = pdfminer.high_level.extract_text(pdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.602439Z",
     "start_time": "2020-12-19T15:06:46.589446Z"
    }
   },
   "outputs": [],
   "source": [
    "original_text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.633424Z",
     "start_time": "2020-12-19T15:06:46.613434Z"
    }
   },
   "outputs": [],
   "source": [
    "text = text.replace(\"\\t\", \" \")\n",
    "text = text.replace(\"\\t\\r\", \" \")\n",
    "text = text.replace(\"\\xa0\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.664787Z",
     "start_time": "2020-12-19T15:06:46.643415Z"
    }
   },
   "outputs": [],
   "source": [
    "paragraphs = text.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.710769Z",
     "start_time": "2020-12-19T15:06:46.671788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unpaired Image-to-Image Translation\\nusing Cycle-Consistent Adversarial Networks',\n",
       " 'Jun-Yan Zhu‚àó',\n",
       " 'Taesung Park‚àó',\n",
       " 'Phillip Isola',\n",
       " 'Alexei A. Efros',\n",
       " 'Berkeley AI Research (BAIR) laboratory, UC Berkeley',\n",
       " '0\\n2\\n0\\n2\\n \\ng\\nu\\nA\\n \\n4\\n2\\n \\n \\n]',\n",
       " 'V\\nC\\n.\\ns\\nc\\n[\\n \\n \\n7\\nv\\n3\\n9\\n5\\n0\\n1\\n.\\n3\\n0\\n7\\n1\\n:\\nv\\ni\\nX\\nr\\na',\n",
       " 'Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically ‚Äútranslate‚Äù an image\\nfrom one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses\\nfrom ImageNet; (right) summer and winter Yosemite photos from Flickr. Example application (bottom): using a collection\\nof paintings of famous artists, our method learns to render natural photographs into the respective styles.',\n",
       " 'Abstract',\n",
       " '1. Introduction',\n",
       " 'Image-to-image translation is a class of vision and\\ngraphics problems where the goal is to learn the mapping\\nbetween an input image and an output image using a train-\\ning set of aligned image pairs. However, for many tasks,\\npaired training data will not be available. We present an\\napproach for learning to translate an image from a source\\ndomain X to a target domain Y in the absence of paired\\nexamples. Our goal is to learn a mapping G : X ‚Üí Y\\nsuch that the distribution of images from G(X) is indistin-\\nguishable from the distribution Y using an adversarial loss.\\nBecause this mapping is highly under-constrained, we cou-\\nple it with an inverse mapping F : Y ‚Üí X and introduce a\\ncycle consistency loss to enforce F (G(X)) ‚âà X (and vice\\nversa). Qualitative results are presented on several tasks\\nwhere paired training data does not exist, including collec-\\ntion style transfer, object transÔ¨Åguration, season transfer,\\nphoto enhancement, etc. Quantitative comparisons against\\nseveral prior methods demonstrate the superiority of our\\napproach.',\n",
       " 'What did Claude Monet see as he placed his easel by the\\nbank of the Seine near Argenteuil on a lovely spring day\\nin 1873 (Figure 1, top-left)? A color photograph, had it\\nbeen invented, may have documented a crisp blue sky and\\na glassy river reÔ¨Çecting it. Monet conveyed his impression\\nof this same scene through wispy brush strokes and a bright\\npalette.',\n",
       " 'What if Monet had happened upon the little harbor in\\nCassis on a cool summer evening (Figure 1, bottom-left)?\\nA brief stroll through a gallery of Monet paintings makes it\\npossible to imagine how he would have rendered the scene:\\nperhaps in pastel shades, with abrupt dabs of paint, and a\\nsomewhat Ô¨Çattened dynamic range.',\n",
       " 'We can imagine all this despite never having seen a side\\nby side example of a Monet painting next to a photo of the\\nscene he painted. Instead, we have knowledge of the set of\\nMonet paintings and of the set of landscape photographs.\\nWe can reason about the stylistic differences between these',\n",
       " '* indicates equal contribution',\n",
       " '1',\n",
       " 'ZebrasHorseshorse        zebrazebra        horseSummer Wintersummer        winterwinter        summerPhotographVan GoghCezanneMonetUkiyo-eMonet        PhotosMonet        photophoto       Monet\\x0ca mapping G : X ‚Üí Y such that the output ÀÜy = G(x),\\nx ‚àà X, is indistinguishable from images y ‚àà Y by an ad-\\nversary trained to classify ÀÜy apart from y. In theory, this ob-\\njective can induce an output distribution over ÀÜy that matches\\nthe empirical distribution pdata(y) (in general, this requires\\nG to be stochastic) [16]. The optimal G thereby translates\\nthe domain X to a domain ÀÜY distributed identically to Y .\\nHowever, such a translation does not guarantee that an in-\\ndividual input x and output y are paired up in a meaningful\\nway ‚Äì there are inÔ¨Ånitely many mappings G that will in-\\nduce the same distribution over ÀÜy. Moreover, in practice,\\nwe have found it difÔ¨Åcult to optimize the adversarial objec-\\ntive in isolation: standard procedures often lead to the well-\\nknown problem of mode collapse, where all input images\\nmap to the same output image and the optimization fails to\\nmake progress [15].',\n",
       " 'These issues call for adding more structure to our ob-\\njective. Therefore, we exploit the property that translation\\nshould be ‚Äúcycle consistent‚Äù, in the sense that if we trans-\\nlate, e.g., a sentence from English to French, and then trans-\\nlate it back from French to English, we should arrive back\\nat the original sentence [3]. Mathematically, if we have a\\ntranslator G : X ‚Üí Y and another translator F : Y ‚Üí X,\\nthen G and F should be inverses of each other, and both\\nmappings should be bijections. We apply this structural as-\\nsumption by training both the mapping G and F simultane-\\nously, and adding a cycle consistency loss [64] that encour-\\nages F (G(x)) ‚âà x and G(F (y)) ‚âà y. Combining this loss\\nwith adversarial losses on domains X and Y yields our full\\nobjective for unpaired image-to-image translation.',\n",
       " 'We apply our method to a wide range of applications,\\nincluding collection style transfer, object transÔ¨Åguration,\\nseason transfer and photo enhancement. We also compare\\nagainst previous approaches that rely either on hand-deÔ¨Åned\\nfactorizations of style and content, or on shared embed-\\nding functions, and show that our method outperforms these\\nbaselines. We provide both PyTorch and Torch implemen-\\ntations. Check out more results at our website.',\n",
       " '2. Related work',\n",
       " 'Generative Adversarial Networks (GANs) [16, 63]\\nhave achieved impressive results in image generation [6,\\n39], image editing [66], and representation learning [39, 43,\\n37]. Recent methods adopt the same idea for conditional\\nimage generation applications, such as text2image [41], im-\\nage inpainting [38], and future prediction [36], as well as to\\nother domains like videos [54] and 3D data [57]. The key to\\nGANs‚Äô success is the idea of an adversarial loss that forces\\nthe generated images to be, in principle, indistinguishable\\nfrom real photos. This loss is particularly powerful for im-\\nage generation tasks, as this is exactly the objective that\\nmuch of computer graphics aims to optimize. We adopt an\\nadversarial loss to learn the mapping such that the translated',\n",
       " 'Figure 2: Paired training data (left) consists of training ex-\\namples {xi, yi}N\\ni=1, where the correspondence between xi\\nand yi exists [22]. We instead consider unpaired training\\ndata (right), consisting of a source set {xi}N\\ni=1 (xi ‚àà X)\\nand a target set {yj}M\\nj=1 (yj ‚àà Y ), with no information pro-\\nvided as to which xi matches which yj.',\n",
       " 'two sets, and thereby imagine what a scene might look like\\nif we were to ‚Äútranslate‚Äù it from one set into the other.',\n",
       " 'In this paper, we present a method that can learn to do the\\nsame: capturing special characteristics of one image col-\\nlection and Ô¨Åguring out how these characteristics could be\\ntranslated into the other image collection, all in the absence\\nof any paired training examples.',\n",
       " 'This problem can be more broadly described as image-\\nto-image translation [22], converting an image from one\\nrepresentation of a given scene, x,\\nto another, y, e.g.,\\ngrayscale to color, image to semantic labels, edge-map to\\nphotograph. Years of research in computer vision, image\\nprocessing, computational photography, and graphics have\\nproduced powerful translation systems in the supervised\\nsetting, where example image pairs {xi, yi}N\\ni=1 are avail-\\nable (Figure 2, left), e.g., [11, 19, 22, 23, 28, 33, 45, 56, 58,\\n62]. However, obtaining paired training data can be difÔ¨Åcult\\nand expensive. For example, only a couple of datasets ex-\\nist for tasks like semantic segmentation (e.g., [4]), and they\\nare relatively small. Obtaining input-output pairs for graph-\\nics tasks like artistic stylization can be even more difÔ¨Åcult\\nsince the desired output is highly complex, typically requir-\\ning artistic authoring. For many tasks, like object transÔ¨Ågu-\\nration (e.g., zebra‚Üîhorse, Figure 1 top-middle), the desired\\noutput is not even well-deÔ¨Åned.',\n",
       " 'We therefore seek an algorithm that can learn to trans-\\nlate between domains without paired input-output examples\\n(Figure 2, right). We assume there is some underlying rela-\\ntionship between the domains ‚Äì for example, that they are\\ntwo different renderings of the same underlying scene ‚Äì and\\nseek to learn that relationship. Although we lack supervi-\\nsion in the form of paired examples, we can exploit super-\\nvision at the level of sets: we are given one set of images in\\ndomain X and a different set in domain Y . We may train',\n",
       " '‚Ä¶‚Ä¶‚Ä¶PairedUnpaired\\x0cFigure 3: (a) Our model contains two mapping functions G : X ‚Üí Y and F : Y ‚Üí X, and associated adversarial\\ndiscriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa\\nfor DX and F . To further regularize the mappings, we introduce two cycle consistency losses that capture the intuition that if\\nwe translate from one domain to the other and back again we should arrive at where we started: (b) forward cycle-consistency\\nloss: x ‚Üí G(x) ‚Üí F (G(x)) ‚âà x, and (c) backward cycle-consistency loss: y ‚Üí F (y) ‚Üí G(F (y)) ‚âà y',\n",
       " 'images cannot be distinguished from images in the target\\ndomain.',\n",
       " 'Image-to-Image Translation The idea of image-to-\\nimage translation goes back at least to Hertzmann et al.‚Äôs\\nImage Analogies [19], who employ a non-parametric tex-\\nture model [10] on a single input-output training image pair.\\nMore recent approaches use a dataset of input-output exam-\\nples to learn a parametric translation function using CNNs\\n(e.g., [33]). Our approach builds on the ‚Äúpix2pix‚Äù frame-\\nwork of Isola et al. [22], which uses a conditional generative\\nadversarial network [16] to learn a mapping from input to\\noutput images. Similar ideas have been applied to various\\ntasks such as generating photographs from sketches [44] or\\nfrom attribute and semantic layouts [25]. However, unlike\\nthe above prior work, we learn the mapping without paired\\ntraining examples.',\n",
       " 'Unpaired Image-to-Image Translation Several other\\nmethods also tackle the unpaired setting, where the goal is\\nto relate two data domains: X and Y . Rosales et al. [42]\\npropose a Bayesian framework that includes a prior based\\non a patch-based Markov random Ô¨Åeld computed from a\\nsource image and a likelihood term obtained from multiple\\nstyle images. More recently, CoGAN [32] and cross-modal\\nscene networks [1] use a weight-sharing strategy to learn a\\ncommon representation across domains. Concurrent to our\\nmethod, Liu et al. [31] extends the above framework with\\na combination of variational autoencoders [27] and genera-\\ntive adversarial networks [16]. Another line of concurrent\\nwork [46, 49, 2] encourages the input and output to share\\nspeciÔ¨Åc ‚Äúcontent‚Äù features even though they may differ in\\n‚Äústyle‚Äú. These methods also use adversarial networks, with\\nadditional terms to enforce the output to be close to the input\\nin a predeÔ¨Åned metric space, such as class label space [2],\\nimage pixel space [46], and image feature space [49].',\n",
       " 'Unlike the above approaches, our formulation does not\\nrely on any task-speciÔ¨Åc, predeÔ¨Åned similarity function be-',\n",
       " 'tween the input and output, nor do we assume that the input\\nand output have to lie in the same low-dimensional embed-\\nding space. This makes our method a general-purpose solu-\\ntion for many vision and graphics tasks. We directly com-\\npare against several prior and contemporary approaches in\\nSection 5.1.',\n",
       " 'Cycle Consistency The idea of using transitivity as a\\nway to regularize structured data has a long history.\\nIn\\nvisual tracking, enforcing simple forward-backward con-\\nsistency has been a standard trick for decades [24, 48].\\nIn the language domain, verifying and improving transla-\\ntions via ‚Äúback translation and reconciliation‚Äù is a technique\\nused by human translators [3] (including, humorously, by\\nMark Twain [51]), as well as by machines [17]. More\\nrecently, higher-order cycle consistency has been used in\\nstructure from motion [61], 3D shape matching [21], co-\\nsegmentation [55], dense semantic alignment [65, 64], and\\ndepth estimation [14]. Of these, Zhou et al. [64] and Go-\\ndard et al. [14] are most similar to our work, as they use a\\ncycle consistency loss as a way of using transitivity to su-\\npervise CNN training. In this work, we are introducing a\\nsimilar loss to push G and F to be consistent with each\\nother. Concurrent with our work, in these same proceed-\\nings, Yi et al. [59] independently use a similar objective\\nfor unpaired image-to-image translation, inspired by dual\\nlearning in machine translation [17].',\n",
       " 'Neural Style Transfer [13, 23, 52, 12] is another way\\nto perform image-to-image translation, which synthesizes a\\nnovel image by combining the content of one image with\\nthe style of another image (typically a painting) based on\\nmatching the Gram matrix statistics of pre-trained deep fea-\\ntures. Our primary focus, on the other hand, is learning\\nthe mapping between two image collections, rather than be-\\ntween two speciÔ¨Åc images, by trying to capture correspon-\\ndences between higher-level appearance structures. There-\\nfore, our method can be applied to other tasks, such as',\n",
       " 'XYGFDYDXGFÀÜYXY(XY(GFÀÜX(a)(b)(c)cycle-consistencylosscycle-consistencylossDYDXÀÜyÀÜxxy\\x0cpainting‚Üí photo, object transÔ¨Åguration, etc. where single\\nsample transfer methods do not perform well. We compare\\nthese two methods in Section 5.2.',\n",
       " '3. Formulation',\n",
       " 'Our goal is to learn mapping functions between two\\ndomains X and Y given training samples {xi}N\\ni=1 where\\nj=1 where yj ‚àà Y 1. We denote the data\\nxi ‚àà X and {yj}M\\ndistribution as x ‚àº pdata(x) and y ‚àº pdata(y). As illus-\\ntrated in Figure 3 (a), our model includes two mappings\\nG : X ‚Üí Y and F : Y ‚Üí X.\\nIn addition, we in-\\ntroduce two adversarial discriminators DX and DY , where\\nDX aims to distinguish between images {x} and translated\\nimages {F (y)}; in the same way, DY aims to discriminate\\nbetween {y} and {G(x)}. Our objective contains two types\\nof terms: adversarial losses [16] for matching the distribu-\\ntion of generated images to the data distribution in the target\\ndomain; and cycle consistency losses to prevent the learned\\nmappings G and F from contradicting each other.',\n",
       " '3.1. Adversarial Loss',\n",
       " 'We apply adversarial losses [16] to both mapping func-\\ntions. For the mapping function G : X ‚Üí Y and its dis-\\ncriminator DY , we express the objective as:',\n",
       " 'LGAN(G, DY , X, Y ) = E\\n+ E',\n",
       " 'y‚àºpdata(y)[log DY (y)]\\nx‚àºpdata(x)[log(1 ‚àí DY (G(x))],',\n",
       " '(1)',\n",
       " 'where G tries to generate images G(x) that look similar to\\nimages from domain Y , while DY aims to distinguish be-\\ntween translated samples G(x) and real samples y. G aims\\nto minimize this objective against an adversary D that tries\\ni.e., minG maxDY LGAN(G, DY , X, Y ).\\nto maximize it,\\nWe introduce a similar adversarial loss for the mapping\\nfunction F : Y ‚Üí X and its discriminator DX as well:\\ni.e., minF maxDX LGAN(F, DX , Y, X).',\n",
       " '3.2. Cycle Consistency Loss',\n",
       " 'Adversarial training can, in theory, learn mappings G\\nand F that produce outputs identically distributed as target\\ndomains Y and X respectively (strictly speaking, this re-\\nquires G and F to be stochastic functions) [15]. However,\\nwith large enough capacity, a network can map the same\\nset of input images to any random permutation of images in\\nthe target domain, where any of the learned mappings can\\ninduce an output distribution that matches the target dis-\\ntribution. Thus, adversarial losses alone cannot guarantee\\nthat the learned function can map an individual input xi to\\na desired output yi. To further reduce the space of possi-\\nble mapping functions, we argue that the learned mapping',\n",
       " '1We often omit the subscript i and j for simplicity.',\n",
       " 'Figure 4: The input images x, output images G(x) and the\\nreconstructed images F (G(x)) from various experiments.\\nFrom top to bottom: photo‚ÜîCezanne, horses‚Üîzebras,\\nwinter‚Üísummer Yosemite, aerial photos‚ÜîGoogle maps.',\n",
       " 'functions should be cycle-consistent: as shown in Figure 3\\n(b), for each image x from domain X, the image translation\\ncycle should be able to bring x back to the original image,\\ni.e., x ‚Üí G(x) ‚Üí F (G(x)) ‚âà x. We call this forward cy-\\ncle consistency. Similarly, as illustrated in Figure 3 (c), for\\neach image y from domain Y , G and F should also satisfy\\nbackward cycle consistency: y ‚Üí F (y) ‚Üí G(F (y)) ‚âà y.\\nWe incentivize this behavior using a cycle consistency loss:',\n",
       " 'Lcyc(G, F ) = E\\n+ E',\n",
       " 'x‚àºpdata(x)[(cid:107)F (G(x)) ‚àí x(cid:107)1]\\ny‚àºpdata(y)[(cid:107)G(F (y)) ‚àí y(cid:107)1].',\n",
       " '(2)',\n",
       " 'In preliminary experiments, we also tried replacing the L1\\nnorm in this loss with an adversarial loss between F (G(x))\\nand x, and between G(F (y)) and y, but did not observe\\nimproved performance.',\n",
       " 'The behavior induced by the cycle consistency loss can\\nbe observed in Figure 4: the reconstructed images F (G(x))\\nend up matching closely to the input images x.',\n",
       " '3.3. Full Objective',\n",
       " 'Our full objective is:',\n",
       " 'L(G, F, DX , DY ) =LGAN(G, DY , X, Y )\\n+ LGAN(F, DX , Y, X)\\n+ ŒªLcyc(G, F ),',\n",
       " '(3)',\n",
       " 'Input ùë•Outputùê∫(ùë•)ReconstructionF(ùê∫ùë•)\\x0cwhere Œª controls the relative importance of the two objec-\\ntives. We aim to solve:',\n",
       " 'G‚àó, F ‚àó = arg min\\nG,F',\n",
       " 'max\\nDx,DY',\n",
       " 'L(G, F, DX , DY ).',\n",
       " '(4)',\n",
       " 'Notice that our model can be viewed as training two ‚Äúau-\\ntoencoders‚Äù [20]: we learn one autoencoder F ‚ó¶ G : X ‚Üí\\nX jointly with another G ‚ó¶ F : Y ‚Üí Y . However, these au-\\ntoencoders each have special internal structures: they map\\nan image to itself via an intermediate representation that\\nis a translation of the image into another domain. Such a\\nsetup can also be seen as a special case of ‚Äúadversarial au-\\ntoencoders‚Äù [34], which use an adversarial loss to train the\\nbottleneck layer of an autoencoder to match an arbitrary tar-\\nget distribution. In our case, the target distribution for the\\nX ‚Üí X autoencoder is that of the domain Y .',\n",
       " 'In Section 5.1.4, we compare our method against ab-\\nlations of the full objective, including the adversarial loss\\nLGAN alone and the cycle consistency loss Lcyc alone, and\\nempirically show that both objectives play critical roles\\nin arriving at high-quality results. We also evaluate our\\nmethod with only cycle loss in one direction and show that\\na single cycle is not sufÔ¨Åcient to regularize the training for\\nthis under-constrained problem.',\n",
       " '4. Implementation',\n",
       " 'Network Architecture We adopt the architecture for our\\ngenerative networks from Johnson et al. [23] who have\\nshown impressive results for neural style transfer and super-\\nresolution. This network contains three convolutions, sev-\\neral residual blocks [18], two fractionally-strided convo-\\nlutions with stride 1\\n2 , and one convolution that maps fea-\\ntures to RGB. We use 6 blocks for 128 √ó 128 images and 9\\nblocks for 256 √ó 256 and higher-resolution training images.\\nSimilar to Johnson et al. [23], we use instance normaliza-\\ntion [53]. For the discriminator networks we use 70 √ó 70\\nPatchGANs [22, 30, 29], which aim to classify whether\\n70 √ó 70 overlapping image patches are real or fake. Such a\\npatch-level discriminator architecture has fewer parameters\\nthan a full-image discriminator and can work on arbitrarily-\\nsized images in a fully convolutional fashion [22].',\n",
       " 'Training details We apply two techniques from recent\\nworks to stabilize our model training procedure. First,\\nfor LGAN (Equation 1), we replace the negative log like-\\nlihood objective by a least-squares loss [35]. This loss is\\nmore stable during training and generates higher quality\\nresults. In particular, for a GAN loss LGAN(G, D, X, Y ),\\nwe train the G to minimize E\\nx‚àºpdata(x)[(D(G(x)) ‚àí 1)2]\\nand train the D to minimize E\\ny‚àºpdata(y)[(D(y) ‚àí 1)2] +\\nE',\n",
       " 'x‚àºpdata(x)[D(G(x))2].\\nSecond, to reduce model oscillation [15], we follow\\nShrivastava et al.‚Äôs strategy [46] and update the discrimi-',\n",
       " 'nators using a history of generated images rather than the\\nones produced by the latest generators. We keep an image\\nbuffer that stores the 50 previously created images.',\n",
       " 'For all the experiments, we set Œª = 10 in Equation 3.\\nWe use the Adam solver [26] with a batch size of 1. All\\nnetworks were trained from scratch with a learning rate of\\n0.0002. We keep the same learning rate for the Ô¨Årst 100\\nepochs and linearly decay the rate to zero over the next 100\\nepochs. Please see the appendix (Section 7) for more details\\nabout the datasets, architectures, and training procedures.',\n",
       " '5. Results',\n",
       " 'We Ô¨Årst compare our approach against recent methods\\nfor unpaired image-to-image translation on paired datasets\\nwhere ground truth input-output pairs are available for eval-\\nuation. We then study the importance of both the adversar-\\nial loss and the cycle consistency loss and compare our full\\nmethod against several variants. Finally, we demonstrate\\nthe generality of our algorithm on a wide range of applica-\\ntions where paired data does not exist. For brevity, we refer\\nto our method as CycleGAN. The PyTorch and Torch code,\\nmodels, and full results can be found at our website.',\n",
       " '5.1. Evaluation',\n",
       " 'Using the same evaluation datasets and metrics as\\n‚Äúpix2pix‚Äù [22], we compare our method against several\\nbaselines both qualitatively and quantitatively. The tasks in-\\nclude semantic labels‚Üîphoto on the Cityscapes dataset [4],\\nand map‚Üîaerial photo on data scraped from Google Maps.\\nWe also perform ablation study on the full loss function.',\n",
       " '5.1.1 Evaluation Metrics',\n",
       " 'AMT perceptual studies On the map‚Üîaerial photo\\ntask, we run ‚Äúreal vs fake‚Äù perceptual studies on Amazon\\nMechanical Turk (AMT) to assess the realism of our out-\\nputs. We follow the same perceptual study protocol from\\nIsola et al. [22], except we only gather data from 25 partic-\\nipants per algorithm we tested. Participants were shown a\\nsequence of pairs of images, one a real photo or map and\\none fake (generated by our algorithm or a baseline), and\\nasked to click on the image they thought was real. The Ô¨Årst\\n10 trials of each session were practice and feedback was\\ngiven as to whether the participant‚Äôs response was correct\\nor incorrect. The remaining 40 trials were used to assess\\nthe rate at which each algorithm fooled participants. Each\\nsession only tested a single algorithm, and participants were\\nonly allowed to complete a single session. The numbers we\\nreport here are not directly comparable to those in [22] as\\nour ground truth images were processed slightly differently\\n2 and the participant pool we tested may be differently dis-',\n",
       " '2We train all the models on 256 √ó 256 images while in pix2pix [22],\\nthe model was trained on 256 √ó 256 patches of 512 √ó 512 images, and',\n",
       " '\\x0cFigure 5: Different methods for mapping labels‚Üîphotos trained on Cityscapes images. From left to right:\\ninput, Bi-\\nGAN/ALI [7, 9], CoGAN [32], feature loss + GAN, SimGAN [46], CycleGAN (ours), pix2pix [22] trained on paired data,\\nand ground truth.',\n",
       " 'Figure 6: Different methods for mapping aerial photos‚Üîmaps on Google Maps. From left to right: input, BiGAN/ALI [7, 9],\\nCoGAN [32], feature loss + GAN, SimGAN [46], CycleGAN (ours), pix2pix [22] trained on paired data, and ground truth.',\n",
       " 'tributed from those tested in [22] (due to running the exper-\\niment at a different date and time). Therefore, our numbers\\nshould only be used to compare our current method against\\nthe baselines (which were run under identical conditions),\\nrather than against [22].',\n",
       " 'FCN score Although perceptual studies may be the gold\\nstandard for assessing graphical realism, we also seek an\\nautomatic quantitative measure that does not require human\\nexperiments. For this, we adopt the ‚ÄúFCN score‚Äù from [22],\\nand use it to evaluate the Cityscapes labels‚Üíphoto task.\\nThe FCN metric evaluates how interpretable the generated\\nphotos are according to an off-the-shelf semantic segmen-\\ntation algorithm (the fully-convolutional network, FCN,\\nfrom [33]). The FCN predicts a label map for a generated\\nphoto. This label map can then be compared against the\\ninput ground truth labels using standard semantic segmen-',\n",
       " 'run convolutionally on the 512 √ó 512 images at test time. We choose\\n256 √ó 256 in our experiments as many baselines cannot scale up to high-\\nresolution images, and CoGAN cannot be tested fully convolutionally.',\n",
       " 'tation metrics described below. The intuition is that if we\\ngenerate a photo from a label map of ‚Äúcar on the road‚Äù,\\nthen we have succeeded if the FCN applied to the generated\\nphoto detects ‚Äúcar on the road‚Äù.',\n",
       " 'Semantic segmentation metrics To evaluate the perfor-\\nmance of photo‚Üílabels, we use the standard metrics from\\nthe Cityscapes benchmark [4], including per-pixel accuracy,\\nper-class accuracy, and mean class Intersection-Over-Union\\n(Class IOU) [4].',\n",
       " '5.1.2 Baselines',\n",
       " 'CoGAN [32] This method learns one GAN generator for\\ndomain X and one for domain Y , with tied weights on the\\nÔ¨Årst few layers for shared latent representations. Translation\\nfrom X to Y can be achieved by Ô¨Ånding a latent represen-\\ntation that generates image X and then rendering this latent\\nrepresentation into style Y .',\n",
       " 'SimGAN [46] Like our method, Shrivastava et al.[46]\\nuses an adversarial loss to train a translation from X to Y .',\n",
       " 'InputBiGANCoGANfeature loss GANSimGANCycleGANpix2pixGround truthInputBiGANCoGANfeature loss GANSimGANCycleGANpix2pixGround truth\\x0cTable 1: AMT ‚Äúreal vs fake‚Äù test on maps‚Üîaerial photos at\\n256 √ó 256 resolution.',\n",
       " 'Table 4: Ablation study: FCN-scores for different variants\\nof our method, evaluated on Cityscapes labels‚Üíphoto.',\n",
       " 'Per-pixel acc. Per-class acc. Class IOU',\n",
       " 'Per-pixel acc. Per-class acc. Class IOU',\n",
       " 'Loss\\nCoGAN [32]\\nBiGAN/ALI [9, 7]\\nSimGAN [46]\\nFeature loss + GAN\\nCycleGAN (ours)',\n",
       " 'Map ‚Üí Photo',\n",
       " 'Photo ‚Üí Map',\n",
       " '% Turkers labeled real % Turkers labeled real',\n",
       " '0.6% ¬± 0.5%\\n2.1% ¬± 1.0%\\n0.7% ¬± 0.5%\\n1.2% ¬± 0.6%\\n26.8% ¬± 2.8%',\n",
       " '0.9% ¬± 0.5%\\n1.9% ¬± 0.9%\\n2.6% ¬± 1.1%\\n0.3% ¬± 0.2%\\n23.2% ¬± 3.4%',\n",
       " 'Loss\\nCoGAN [32]\\nBiGAN/ALI [9, 7]\\nSimGAN [46]\\nFeature loss + GAN\\nCycleGAN (ours)\\npix2pix [22]',\n",
       " 'Loss\\nCoGAN [32]\\nBiGAN/ALI [9, 7]\\nSimGAN [46]\\nFeature loss + GAN\\nCycleGAN (ours)\\npix2pix [22]',\n",
       " '0.40\\n0.19\\n0.20\\n0.06\\n0.52\\n0.71',\n",
       " '0.45\\n0.41\\n0.47\\n0.50\\n0.58\\n0.85',\n",
       " '0.10\\n0.06\\n0.10\\n0.04\\n0.17\\n0.25',\n",
       " '0.11\\n0.13\\n0.11\\n0.10\\n0.22\\n0.40',\n",
       " '0.06\\n0.02\\n0.04\\n0.01\\n0.11\\n0.18',\n",
       " '0.08\\n0.07\\n0.07\\n0.06\\n0.16\\n0.32',\n",
       " 'Table 2: FCN-scores for different methods, evaluated on\\nCityscapes labels‚Üíphoto.',\n",
       " 'Per-pixel acc. Per-class acc. Class IOU',\n",
       " 'Table 3: ClassiÔ¨Åcation performance of photo‚Üílabels for\\ndifferent methods on cityscapes.',\n",
       " 'The regularization term (cid:107)x ‚àí G(x)(cid:107)1 i s used to penalize\\nmaking large changes at pixel level.',\n",
       " 'Feature loss + GAN We also test a variant of Sim-\\nGAN [46] where the L1 loss is computed over deep\\nimage features using a pretrained network (VGG-16\\nrelu4 2 [47]), rather than over RGB pixel values. Com-\\nputing distances in deep feature space, like this, is also\\nsometimes referred to as using a ‚Äúperceptual loss‚Äù [8, 23].',\n",
       " 'BiGAN/ALI [9, 7] Unconditional GANs [16] learn a\\ngenerator G : Z ‚Üí X, that maps a random noise z to an\\nimage x. The BiGAN [9] and ALI [7] propose to also learn\\nthe inverse mapping function F : X ‚Üí Z. Though they\\nwere originally designed for mapping a latent vector z to an\\nimage x, we implemented the same objective for mapping a\\nsource image x to a target image y.',\n",
       " 'pix2pix [22] We also compare against pix2pix [22],\\nwhich is trained on paired data, to see how close we can\\nget to this ‚Äúupper bound‚Äù without using any paired data.',\n",
       " 'For a fair comparison, we implement all the baselines\\nusing the same architecture and details as our method, ex-\\ncept for CoGAN [32]. CoGAN builds on generators that\\nproduce images from a shared latent representation, which\\nis incompatible with our image-to-image network. We use\\nthe public implementation of CoGAN instead.',\n",
       " '5.1.3 Comparison against baselines',\n",
       " 'As can be seen in Figure 5 and Figure 6, we were unable to\\nachieve compelling results with any of the baselines. Our',\n",
       " 'Per-pixel acc. Per-class acc. Class IOU',\n",
       " 'Loss\\nCycle alone\\nGAN alone\\nGAN + forward cycle\\nGAN + backward cycle\\nCycleGAN (ours)',\n",
       " 'Loss\\nCycle alone\\nGAN alone\\nGAN + forward cycle\\nGAN + backward cycle\\nCycleGAN (ours)',\n",
       " '0.22\\n0.51\\n0.55\\n0.39\\n0.52',\n",
       " '0.10\\n0.53\\n0.49\\n0.01\\n0.58',\n",
       " '0.07\\n0.11\\n0.18\\n0.14\\n0.17',\n",
       " '0.05\\n0.11\\n0.11\\n0.06\\n0.22',\n",
       " '0.02\\n0.08\\n0.12\\n0.06\\n0.11',\n",
       " '0.02\\n0.07\\n0.07\\n0.01\\n0.16',\n",
       " 'Table 5: Ablation study: classiÔ¨Åcation performance of\\nphoto‚Üílabels for different losses, evaluated on Cityscapes.',\n",
       " 'method, on the other hand, can produce translations that are\\noften of similar quality to the fully supervised pix2pix.',\n",
       " 'Table 1 reports performance regarding the AMT per-\\nceptual realism task. Here, we see that our method can\\nfool participants on around a quarter of trials, in both the\\nmaps‚Üíaerial photos direction and the aerial photos‚Üímaps\\ndirection at 256 √ó 256 resolution3. All the baselines almost\\nnever fooled participants.',\n",
       " 'Table 2 assesses the performance of the labels‚Üíphoto\\ntask on the Cityscapes and Table 3 evaluates the opposite\\nmapping (photos‚Üílabels). In both cases, our method again\\noutperforms the baselines.',\n",
       " '5.1.4 Analysis of the loss function',\n",
       " 'In Table 4 and Table 5, we compare against ablations\\nof our full loss. Removing the GAN loss substantially\\ndegrades results, as does removing the cycle-consistency\\nloss. We therefore conclude that both terms are critical\\nto our results. We also evaluate our method with the cy-\\ncle loss in only one direction: GAN + forward cycle loss\\nE\\nx‚àºpdata(x)[(cid:107)F (G(x))‚àíx(cid:107)1], or GAN + backward cycle loss\\nE\\ny‚àºpdata(y)[(cid:107)G(F (y))‚àíy(cid:107)1] (Equation 2) and Ô¨Ånd that it of-\\nten incurs training instability and causes mode collapse, es-\\npecially for the direction of the mapping that was removed.\\nFigure 7 shows several qualitative examples.',\n",
       " '5.1.5',\n",
       " 'Image reconstruction quality',\n",
       " 'In Figure 4, we show a few random samples of the recon-\\nstructed images F (G(x)). We observed that the recon-\\nstructed images were often close to the original inputs x,\\nat both training and testing time, even in cases where one\\ndomain represents signiÔ¨Åcantly more diverse information,\\nsuch as map‚Üîaerial photos.',\n",
       " '3We also train CycleGAN and pix2pix at 512 √ó 512 resolution, and\\nobserve the comparable performance: maps‚Üíaerial photos: CycleGAN:\\n37.5% ¬± 3.6% and pix2pix: 33.9% ¬± 3.1%; aerial photos‚Üímaps: Cy-\\ncleGAN: 16.5% ¬± 4.1% and pix2pix: 8.5% ¬± 2.6%',\n",
       " '\\x0cFigure 7: Different variants of our method for mapping labels‚Üîphotos trained on cityscapes. From left to right: input, cycle-\\nconsistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss (F (G(x)) ‚âà x), GAN + backward\\ncycle-consistency loss (G(F (y)) ‚âà y), CycleGAN (our full method), and ground truth. Both Cycle alone and GAN +\\nbackward fail to produce images similar to the target domain. GAN alone and GAN + forward suffer from mode collapse,\\nproducing identical label maps regardless of the input photo.',\n",
       " 'the appendix (Section 7) for more details about the datasets.\\nWe observe that translations on training data are often more\\nappealing than those on test data, and full results of all ap-\\nplications on both training and test data can be viewed on\\nour project website.',\n",
       " 'Collection style transfer (Figure 10 and Figure 11)\\nWe train the model on landscape photographs downloaded\\nfrom Flickr and WikiArt. Unlike recent work on ‚Äúneural\\nstyle transfer‚Äù [13], our method learns to mimic the style\\nof an entire collection of artworks, rather than transferring\\nthe style of a single selected piece of art. Therefore, we\\ncan learn to generate photos in the style of, e.g., Van Gogh,\\nrather than just in the style of Starry Night. The size of the\\ndataset for each artist/style was 526, 1073, 400, and 563 for\\nCezanne, Monet, Van Gogh, and Ukiyo-e.',\n",
       " 'Object transÔ¨Åguration (Figure 13) The model\\nis\\ntrained to translate one object class from ImageNet [5] to\\nanother (each class contains around 1000 training images).\\nTurmukhambetov et al. [50] propose a subspace model to\\ntranslate one object into another object of the same category,\\nwhile our method focuses on object transÔ¨Åguration between\\ntwo visually similar categories.',\n",
       " 'Season transfer (Figure 13) The model is trained on\\n854 winter photos and 1273 summer photos of Yosemite\\ndownloaded from Flickr.',\n",
       " 'Photo generation from paintings (Figure 12) For\\npainting‚Üíphoto, we Ô¨Ånd that it is helpful to introduce an\\nadditional loss to encourage the mapping to preserve color\\ncomposition between the input and output. In particular, we\\nadopt the technique of Taigman et al. [49] and regularize the\\ngenerator to be near an identity mapping when real samples\\nof the target domain are provided as the input to the gen-\\nerator: i.e., Lidentity(G, F ) = E\\ny‚àºpdata(y)[(cid:107)G(y) ‚àí y(cid:107)1] +\\nE',\n",
       " 'x‚àºpdata(x)[(cid:107)F (x) ‚àí x(cid:107)1].',\n",
       " 'Figure 8: Example results of CycleGAN on paired datasets\\nused in ‚Äúpix2pix‚Äù [22] such as architectural labels‚Üîphotos\\nand edges‚Üîshoes.',\n",
       " '5.1.6 Additional results on paired datasets',\n",
       " 'Figure 8 shows some example results on other paired\\ndatasets used in ‚Äúpix2pix‚Äù [22], such as architectural\\nlabels‚Üîphotos from the CMP Facade Database [40], and\\nedges‚Üîshoes from the UT Zappos50K dataset [60]. The\\nimage quality of our results is close to those produced by\\nthe fully supervised pix2pix while our method learns the\\nmapping without paired supervision.',\n",
       " '5.2. Applications',\n",
       " 'We demonstrate our method on several applications\\nwhere paired training data does not exist. Please refer to',\n",
       " 'Ground truthInputGAN aloneCycle aloneGAN+forwardGAN+backwardCycleGANlabel ‚Üífacadefacade ‚Üílabeledges  ‚Üíshoesshoes  ‚ÜíedgesInputOutputInputOutputInputOutput\\x0ccollection, we compute the average Gram Matrix across the\\ntarget domain and use this matrix to transfer the ‚Äúaverage\\nstyle‚Äù with Gatys et al [13].',\n",
       " 'Figure 16 demonstrates similar comparisons for other\\ntranslation tasks. We observe that Gatys et al. [13] requires\\nÔ¨Ånding target style images that closely match the desired\\noutput, but still often fails to produce photorealistic results,\\nwhile our method succeeds to generate natural-looking re-\\nsults, similar to the target domain.',\n",
       " '6. Limitations and Discussion',\n",
       " 'Although our method can achieve compelling results in\\nmany cases, the results are far from uniformly positive. Fig-\\nure 17 shows several typical failure cases. On translation\\ntasks that involve color and texture changes, as many of\\nthose reported above, the method often succeeds. We have\\nalso explored tasks that require geometric changes, with lit-\\ntle success. For example, on the task of dog‚Üícat transÔ¨Ågu-\\nration, the learned translation degenerates into making min-\\nimal changes to the input (Figure 17). This failure might be\\ncaused by our generator architectures which are tailored for\\ngood performance on the appearance changes. Handling\\nmore varied and extreme transformations, especially geo-\\nmetric changes, is an important problem for future work.',\n",
       " 'Some failure cases are caused by the distribution charac-\\nteristics of the training datasets. For example, our method\\nhas got confused in the horse ‚Üí zebra example (Figure 17,\\nright), because our model was trained on the wild horse and\\nzebra synsets of ImageNet, which does not contain images\\nof a person riding a horse or zebra.',\n",
       " 'We also observe a lingering gap between the results\\nachievable with paired training data and those achieved by\\nour unpaired method. In some cases, this gap may be very\\nfor example, our\\nhard ‚Äì or even impossible ‚Äì to close:\\nmethod sometimes permutes the labels for tree and build-\\ning in the output of the photos‚Üílabels task. Resolving this\\nambiguity may require some form of weak semantic super-\\nvision. Integrating weak or semi-supervised data may lead\\nto substantially more powerful translators, still at a fraction\\nof the annotation cost of the fully-supervised systems.',\n",
       " 'Nonetheless, in many cases completely unpaired data is\\nplentifully available and should be made use of. This paper\\npushes the boundaries of what is possible in this ‚Äúunsuper-\\nvised‚Äù setting.',\n",
       " 'Acknowledgments: We thank Aaron Hertzmann, Shiry\\nGinosar, Deepak Pathak, Bryan Russell, Eli Shechtman,\\nRichard Zhang, and Tinghui Zhou for many helpful com-\\nments. This work was supported in part by NSF SMA-\\n1514512, NSF IIS-1633310, a Google Research Award, In-\\ntel Corp, and hardware donations from NVIDIA. JYZ is\\nsupported by the Facebook Graduate Fellowship and TP is\\nsupported by the Samsung Scholarship. The photographs\\nused for style transfer were taken by AE, mostly in France.',\n",
       " 'Figure 9: The effect of the identity mapping loss on Monet‚Äôs\\npainting‚Üí photos. From left to right: input paintings, Cy-\\ncleGAN without identity mapping loss, CycleGAN with\\nidentity mapping loss. The identity mapping loss helps pre-\\nserve the color of the input paintings.',\n",
       " 'Without Lidentity,',\n",
       " 'the generator G and F are free to\\nchange the tint of input images when there is no need to.\\nFor example, when learning the mapping between Monet‚Äôs\\npaintings and Flickr photographs, the generator often maps\\npaintings of daytime to photographs taken during sunset,\\nbecause such a mapping may be equally valid under the ad-\\nversarial loss and cycle consistency loss. The effect of this\\nidentity mapping loss are shown in Figure 9.',\n",
       " 'In Figure 12, we show additional results translating\\nMonet‚Äôs paintings to photographs. This Ô¨Ågure and Figure 9\\nshow results on paintings that were included in the train-\\ning set, whereas for all other experiments in the paper, we\\nonly evaluate and show test set results. Because the training\\nset does not include paired data, coming up with a plausi-\\nble translation for a training set painting is a nontrivial task.\\nIndeed, since Monet is no longer able to create new paint-\\nings, generalization to unseen, ‚Äútest set‚Äù, paintings is not a\\npressing problem.',\n",
       " 'Photo enhancement (Figure 14) We show that our\\nmethod can be used to generate photos with shallower depth\\nof Ô¨Åeld. We train the model on Ô¨Çower photos downloaded\\nfrom Flickr. The source domain consists of Ô¨Çower photos\\ntaken by smartphones, which usually have deep DoF due\\nto a small aperture. The target contains photos captured by\\nDSLRs with a larger aperture. Our model successfully gen-\\nerates photos with shallower depth of Ô¨Åeld from the photos\\ntaken by smartphones.',\n",
       " 'Comparison with Gatys et al. [13] In Figure 15, we\\ncompare our results with neural style transfer [13] on photo\\nstylization. For each row, we Ô¨Årst use two representative\\nartworks as the style images for [13]. Our method, on the\\nother hand, can produce photos in the style of entire collec-\\ntion. To compare against neural style transfer of an entire',\n",
       " 'CycleGANInputCycleGAN+L\"#$%&\"&‚Äô\\x0cFigure 10: Collection style transfer I: we transfer input images into the artistic styles of Monet, Van Gogh, Cezanne, and\\nUkiyo-e. Please see our website for additional examples.',\n",
       " 'Ukiyo-eMonetInputVan GoghCezanne\\x0cFigure 11: Collection style transfer II: we transfer input images into the artistic styles of Monet, Van Gogh, Cezanne, Ukiyo-e.\\nPlease see our website for additional examples.',\n",
       " 'MonetUkiyo-eInputVan GoghCezanne\\x0cFigure 12: Relatively successful results on mapping Monet‚Äôs paintings to a photographic style. Please see our website for\\nadditional examples.',\n",
       " 'InputOutputInputOutput\\x0cFigure 13: Our method applied to several translation problems. These images are selected as relatively successful results\\n‚Äì please see our website for more comprehensive and random results.\\nIn the top two rows, we show results on object\\ntransÔ¨Åguration between horses and zebras, trained on 939 images from the wild horse class and 1177 images from the zebra\\nclass in Imagenet [5]. Also check out the horse‚Üízebra demo video. The middle two rows show results on season transfer,\\ntrained on winter and summer photos of Yosemite from Flickr. In the bottom two rows, we train our method on 996 apple\\nimages and 1020 navel orange images from ImageNet.',\n",
       " 'InputInputInputOutputOutputOutputhorse ‚Üízebrazebra ‚Üíhorsesummer Yosemite ‚Üíwinter Yosemite apple ‚Üíorangeorange ‚Üíapplewinter Yosemite ‚Üísummer Yosemite\\x0cFigure 14: Photo enhancement: mapping from a set of smartphone snaps to professional DSLR photographs, the system often\\nlearns to produce shallow focus. Here we show some of the most successful results in our test set ‚Äì average performance is\\nconsiderably worse. Please see our website for more comprehensive and random examples.',\n",
       " 'Figure 15: We compare our method with neural style transfer [13] on photo stylization. Left to right: input image, results\\nfrom Gatys et al. [13] using two different representative artworks as style images, results from Gatys et al. [13] using the\\nentire collection of the artist, and CycleGAN (ours).',\n",
       " 'InputOutputInputOutputInputOutputInputOutputInputGatyset al. (image I)CycleGANGatyset al. (image II)Gatyset al. (collection)Photo ‚ÜíVan Gogh Photo ‚ÜíUkiyo-ePhoto ‚ÜíCezanne\\x0cFigure 16: We compare our method with neural style transfer [13] on various applications. From top to bottom:\\napple‚Üíorange, horse‚Üízebra, and Monet‚Üíphoto. Left to right: input image, results from Gatys et al. [13] using two different\\nimages as style images, results from Gatys et al. [13] using all the images from the target domain, and CycleGAN (ours).',\n",
       " 'Figure 17: Typical failure cases of our method. Left: in the task of dog‚Üícat transÔ¨Åguration, CycleGAN can only make\\nminimal changes to the input. Right: CycleGAN also fails in this horse ‚Üí zebra example as our model has not seen images\\nof horseback riding during training. Please see our website for more comprehensive results.',\n",
       " 'InputGatyset al. (image I)CycleGANGatyset al. (image II)Gatyset al. (collection)apple ‚Üíorangehorse  ‚ÜízebraMonet ‚ÜíphotoInputOutputInputOutputapple ‚Üíorangezebra ‚Üíhorsedog ‚Üícatcat ‚Üídogwinter ‚ÜísummerMonet ‚Üíphotophoto ‚ÜíUkiyo-ephoto ‚ÜíVan GoghInputOutputiPhone photo ‚ÜíDSLR photohorse‚ÜízebraImageNet‚Äúwildhorse‚ÄùtrainingimagesInputOutput\\x0cReferences',\n",
       " '[1] Y. Aytar, L. Castrejon, C. Vondrick, H. Pirsiavash, and\\nA. Torralba. Cross-modal scene networks. PAMI,\\n2016. 3',\n",
       " '[2] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and\\nD. Krishnan. Unsupervised pixel-level domain adap-\\ntation with generative adversarial networks. In CVPR,\\n2017. 3',\n",
       " '[3] R. W. Brislin. Back-translation for cross-cultural\\nJournal of cross-cultural psychology,',\n",
       " 'research.\\n1(3):185‚Äì216, 1970. 2, 3',\n",
       " '[4] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. En-\\nzweiler, R. Benenson, U. Franke, S. Roth, and\\nB. Schiele. The cityscapes dataset for semantic urban\\nscene understanding. In CVPR, 2016. 2, 5, 6, 18\\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\\nL. Fei-Fei. Imagenet: A large-scale hierarchical im-\\nage database. In CVPR, 2009. 8, 13, 18',\n",
       " '[6] E. L. Denton, S. Chintala, R. Fergus, et al. Deep gen-\\nerative image models using a laplacian pyramid of ad-\\nversarial networks. In NIPS, 2015. 2',\n",
       " '[7] J. Donahue, P. Kr¬®ahenb¬®uhl, and T. Darrell. Adversarial',\n",
       " 'feature learning. In ICLR, 2017. 6, 7',\n",
       " '[8] A. Dosovitskiy and T. Brox. Generating images with\\nperceptual similarity metrics based on deep networks.\\nIn NIPS, 2016. 7',\n",
       " '[9] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Ar-\\njovsky, O. Mastropietro, and A. Courville. Adversari-\\nally learned inference. In ICLR, 2017. 6, 7',\n",
       " 'non-parametric sampling. In ICCV, 1999. 3',\n",
       " '[11] D. Eigen and R. Fergus. Predicting depth, surface nor-\\nmals and semantic labels with a common multi-scale\\nconvolutional architecture. In ICCV, 2015. 2',\n",
       " '[12] L. A. Gatys, M. Bethge, A. Hertzmann, and E. Shecht-\\nman. Preserving color in neural artistic style transfer.\\narXiv preprint arXiv:1606.05897, 2016. 3',\n",
       " '[13] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style\\ntransfer using convolutional neural networks. CVPR,\\n2016. 3, 8, 9, 14, 15',\n",
       " '[14] C. Godard, O. Mac Aodha, and G. J. Brostow. Un-\\nsupervised monocular depth estimation with left-right\\nconsistency. In CVPR, 2017. 3',\n",
       " '[15] I. Goodfellow. NIPS 2016 tutorial: Generative ad-\\nversarial networks. arXiv preprint arXiv:1701.00160,\\n2016. 2, 4, 5',\n",
       " '[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\\ngio. Generative adversarial nets. In NIPS, 2014. 2, 3,\\n4, 7',\n",
       " '[17] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and\\nW.-Y. Ma. Dual learning for machine translation. In\\nNIPS, 2016. 3',\n",
       " '[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\\nlearning for image recognition. In CVPR, 2016. 5',\n",
       " '[19] A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and\\nD. H. Salesin. Image analogies. In SIGGRAPH, 2001.\\n2, 3',\n",
       " '[20] G. E. Hinton and R. R. Salakhutdinov. Reducing the\\ndimensionality of data with neural networks. Science,\\n313(5786):504‚Äì507, 2006. 5',\n",
       " '[21] Q.-X. Huang and L. Guibas. Consistent shape maps\\nvia semideÔ¨Ånite programming. In Symposium on Ge-\\nometry Processing, 2013. 3',\n",
       " '[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-\\nto-image translation with conditional adversarial net-\\nworks. In CVPR, 2017. 2, 3, 5, 6, 7, 8, 18',\n",
       " '[23] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses\\nfor real-time style transfer and super-resolution.\\nIn\\nECCV, 2016. 2, 3, 5, 7, 18',\n",
       " '[24] Z. Kalal, K. Mikolajczyk, and J. Matas. Forward-\\nbackward error: Automatic detection of tracking fail-\\nures. In ICPR, 2010. 3',\n",
       " '[25] L. Karacan, Z. Akata, A. Erdem, and E. Erdem.\\nLearning to generate images of outdoor scenes from\\narXiv preprint\\nattributes and semantic layouts.\\narXiv:1612.00215, 2016. 3',\n",
       " '[26] D. Kingma and J. Ba. Adam: A method for stochastic',\n",
       " '[27] D. P. Kingma and M. Welling. Auto-encoding varia-',\n",
       " 'tional bayes. ICLR, 2014. 3',\n",
       " '[28] P.-Y. Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays.\\nTransient attributes for high-level understanding and\\nediting of outdoor scenes. ACM TOG, 33(4):149,\\n2014. 2',\n",
       " '[29] C. Ledig, L. Theis, F. Husz¬¥ar, J. Caballero, A. Cun-\\nningham, A. Acosta, A. Aitken, A. Tejani, J. Totz,\\nZ. Wang, et al. Photo-realistic single image super-\\nresolution using a generative adversarial network. In\\nCVPR, 2017. 5',\n",
       " '[30] C. Li and M. Wand. Precomputed real-time texture\\nsynthesis with markovian generative adversarial net-\\nworks. ECCV, 2016. 5',\n",
       " '[31] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised\\nimage-to-image translation networks. In NIPS, 2017.\\n3',\n",
       " '[32] M.-Y. Liu and O. Tuzel. Coupled generative adversar-',\n",
       " 'ial networks. In NIPS, 2016. 3, 6, 7',\n",
       " '[10] A. A. Efros and T. K. Leung. Texture synthesis by',\n",
       " 'optimization. In ICLR, 2015. 5',\n",
       " '\\x0c[33] J. Long, E. Shelhamer, and T. Darrell. Fully convolu-\\ntional networks for semantic segmentation. In CVPR,\\n2015. 2, 3, 6',\n",
       " '[34] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and\\nB. Frey. Adversarial autoencoders. In ICLR, 2016. 5\\n[35] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P.\\nSmolley. Least squares generative adversarial net-\\nworks. In CVPR. IEEE, 2017. 5',\n",
       " '[36] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-\\nscale video prediction beyond mean square error. In\\nICLR, 2016. 2',\n",
       " '[37] M. F. Mathieu, J. Zhao, A. Ramesh, P. Sprechmann,\\nand Y. LeCun. Disentangling factors of variation\\nin deep representation using adversarial training.\\nIn\\nNIPS, 2016. 2',\n",
       " '[38] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and\\nA. A. Efros. Context encoders: Feature learning by\\ninpainting. CVPR, 2016. 2',\n",
       " '[39] A. Radford, L. Metz, and S. Chintala. Unsupervised\\nrepresentation learning with deep convolutional gen-\\nerative adversarial networks. In ICLR, 2016. 2\\n[40] R. ÀáS. Radim TyleÀácek. Spatial pattern templates for\\nrecognition of objects with regular structure. In Proc.\\nGCPR, Saarbrucken, Germany, 2013. 8, 18',\n",
       " '[41] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele,\\nand H. Lee. Generative adversarial text to image syn-\\nthesis. In ICML, 2016. 2',\n",
       " '[42] R. Rosales, K. Achan, and B. J. Frey. Unsupervised',\n",
       " 'image translation. In ICCV, 2003. 3',\n",
       " '[43] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung,\\nImproved techniques for',\n",
       " 'A. Radford, and X. Chen.\\ntraining GANs. In NIPS, 2016. 2',\n",
       " '[44] P. Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays. Scrib-\\nbler: Controlling deep image synthesis with sketch\\nand color. In CVPR, 2017. 3',\n",
       " '[45] Y. Shih, S. Paris, F. Durand, and W. T. Freeman. Data-\\ndriven hallucination of different times of day from a\\nsingle outdoor photo. ACM TOG, 32(6):200, 2013. 2\\n[46] A. Shrivastava, T. PÔ¨Åster, O. Tuzel, J. Susskind,\\nW. Wang, and R. Webb. Learning from simulated and\\nunsupervised images through adversarial training. In\\nCVPR, 2017. 3, 5, 6, 7',\n",
       " '[47] K. Simonyan and A. Zisserman. Very deep convolu-\\ntional networks for large-scale image recognition. In\\nICLR, 2015. 7',\n",
       " '[48] N. Sundaram, T. Brox, and K. Keutzer. Dense point\\ntrajectories by gpu-accelerated large displacement op-\\ntical Ô¨Çow. In ECCV, 2010. 3',\n",
       " '[49] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised',\n",
       " 'cross-domain image generation. In ICLR, 2017. 3, 8',\n",
       " '[50] D. Turmukhambetov, N. D. Campbell, S. J. Prince,\\nand J. Kautz. Modeling object appearance using\\nIn CVPR,\\ncontext-conditioned component analysis.\\n2015. 8',\n",
       " '[51] M. Twain. The jumping frog:',\n",
       " 'in english, then in\\nfrench, and then clawed back into a civilized language\\nonce more by patient. Unremunerated Toil, 3, 1903. 3\\n[52] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempit-\\nsky. Texture networks: Feed-forward synthesis of tex-\\ntures and stylized images. In ICML, 2016. 3',\n",
       " '[53] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance\\nnormalization: The missing ingredient for fast styliza-\\ntion. arXiv preprint arXiv:1607.08022, 2016. 5\\n[54] C. Vondrick, H. Pirsiavash, and A. Torralba. Generat-\\ning videos with scene dynamics. In NIPS, 2016. 2',\n",
       " '[55] F. Wang, Q. Huang, and L. J. Guibas.',\n",
       " 'Image co-\\nsegmentation via consistent functional maps. In ICCV,\\n2013. 3',\n",
       " '[56] X. Wang and A. Gupta. Generative image model-\\ning using style and structure adversarial networks. In\\nECCV, 2016. 2',\n",
       " '[57] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenen-\\nbaum. Learning a probabilistic latent space of ob-\\nject shapes via 3d generative-adversarial modeling. In\\nNIPS, 2016. 2',\n",
       " '[58] S. Xie and Z. Tu. Holistically-nested edge detection.',\n",
       " 'In ICCV, 2015. 2',\n",
       " '[59] Z. Yi, H. Zhang, T. Gong, Tan, and M. Gong. Dual-\\ngan: Unsupervised dual learning for image-to-image\\ntranslation. In ICCV, 2017. 3',\n",
       " '[60] A. Yu and K. Grauman. Fine-grained visual compar-\\nisons with local learning. In CVPR, 2014. 8, 18\\n[61] C. Zach, M. Klopschitz, and M. Pollefeys. Disam-\\nbiguating visual relations using loop constraints.\\nIn\\nCVPR, 2010. 3',\n",
       " '[62] R. Zhang, P. Isola, and A. A. Efros. Colorful image',\n",
       " 'colorization. In ECCV, 2016. 2',\n",
       " '[63] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based\\ngenerative adversarial network. In ICLR, 2017. 2\\n[64] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and\\nA. A. Efros. Learning dense correspondence via 3d-\\nguided cycle consistency. In CVPR, 2016. 2, 3\\n[65] T. Zhou, Y. J. Lee, S. Yu, and A. A. Efros. Flowweb:\\nimage set alignment by weaving consistent,',\n",
       " 'Joint\\npixel-wise correspondences. In CVPR, 2015. 3\\n[66] J.-Y. Zhu, P. Kr¬®ahenb¬®uhl, E. Shechtman, and A. A.\\nEfros. Generative visual manipulation on the natural\\nimage manifold. In ECCV, 2016. 2',\n",
       " '\\x0cFlower photo enhancement Flower images taken on\\nsmartphones were downloaded from Flickr by searching for\\nthe photos taken by Apple iPhone 5, 5s, or 6, with search\\ntext Ô¨Çower. DSLR images with shallow DoF were also\\ndownloaded from Flickr by search tag Ô¨Çower, dof. The im-\\nages were scaled to 360 pixels by width. The identity map-\\nping loss of weight 0.5Œª was used. The training set size\\nof the smartphone and DSLR dataset were 1813 and 3326,\\nrespectively. We set Œª = 10.',\n",
       " '7.2. Network architectures',\n",
       " 'We provide both PyTorch and Torch implementations.\\nGenerator architectures We adopt our architectures\\nfrom Johnson et al. [23]. We use 6 residual blocks for\\n128 √ó 128 training images, and 9 residual blocks for 256 √ó\\n256 or higher-resolution training images. Below, we follow\\nthe naming convention used in the Johnson et al.‚Äôs Github\\nrepository.',\n",
       " 'Let c7s1-k denote a 7 √ó 7 Convolution-InstanceNorm-\\nReLU layer with k Ô¨Ålters and stride 1. dk denotes a 3 √ó 3\\nConvolution-InstanceNorm-ReLU layer with k Ô¨Ålters and\\nstride 2. ReÔ¨Çection padding was used to reduce artifacts.\\nRk denotes a residual block that contains two 3 √ó 3 con-\\nvolutional layers with the same number of Ô¨Ålters on both\\nlayer. uk denotes a 3 √ó 3 fractional-strided-Convolution-\\nInstanceNorm-ReLU layer with k Ô¨Ålters and stride 1\\n2 .\\nThe network with 6 residual blocks consists of:\\nc7s1-64,d128,d256,R256,R256,R256,\\nR256,R256,R256,u128,u64,c7s1-3',\n",
       " 'The network with 9 residual blocks consists of:\\nc7s1-64,d128,d256,R256,R256,R256,\\nR256,R256,R256,R256,R256,R256,u128\\nu64,c7s1-3',\n",
       " 'Discriminator architectures For discriminator net-\\nworks, we use 70 √ó 70 PatchGAN [22]. Let Ck denote a\\n4 √ó 4 Convolution-InstanceNorm-LeakyReLU layer with k\\nÔ¨Ålters and stride 2. After the last layer, we apply a convo-\\nlution to produce a 1-dimensional output. We do not use\\nInstanceNorm for the Ô¨Årst C64 layer. We use leaky ReLUs\\nwith a slope of 0.2. The discriminator architecture is:\\nC64-C128-C256-C512',\n",
       " '7. Appendix',\n",
       " '7.1. Training details',\n",
       " 'We train our networks from scratch, with a learning rate\\nof 0.0002. In practice, we divide the objective by 2 while\\noptimizing D, which slows down the rate at which D learns,\\nrelative to the rate of G. We keep the same learning rate\\nfor the Ô¨Årst 100 epochs and linearly decay the rate to zero\\nover the next 100 epochs. Weights are initialized from a\\nGaussian distribution N (0, 0.02).',\n",
       " 'Cityscapes label‚ÜîPhoto 2975 training images from the\\nCityscapes training set [4] with image size 128 √ó 128. We\\nused the Cityscapes val set for testing.',\n",
       " 'Maps‚Üîaerial photograph 1096 training images were\\nscraped from Google Maps [22] with image size 256 √ó 256.\\nImages were sampled from in and around New York City.\\nData was then split into train and test about the median lat-\\nitude of the sampling region (with a buffer region added to\\nensure that no training pixel appeared in the test set).',\n",
       " 'Architectural facades labels‚Üîphoto 400 training im-',\n",
       " 'ages from the CMP Facade Database [40].',\n",
       " 'Edges‚Üíshoes around 50, 000 training images from UT\\nZappos50K dataset [60]. The model was trained for 5\\nepochs.',\n",
       " 'Horse‚ÜîZebra and Apple‚ÜîOrange We downloaded\\nthe images from ImageNet [5] using keywords wild horse,\\nzebra, apple, and navel orange. The images were scaled to\\n256 √ó 256 pixels. The training set size of each class: 939\\n(horse), 1177 (zebra), 996 (apple), and 1020 (orange).',\n",
       " 'Summer‚ÜîWinter Yosemite The images were down-\\nloaded using Flickr API with the tag yosemite and the date-\\ntaken Ô¨Åeld. Black-and-white photos were pruned. The im-\\nages were scaled to 256 √ó 256 pixels. The training size of\\neach class: 1273 (summer) and 854 ( winter).',\n",
       " 'Photo‚ÜîArt for style transfer The art images were\\ndownloaded from Wikiart.org. Some artworks that were\\nsketches or too obscene were pruned by hand. The pho-\\ntos were downloaded from Flickr using the combination\\nof tags landscape and landscapephotography. Black-and-\\nwhite photos were pruned. The images were scaled to\\n256 √ó 256 pixels. The training set size of each class\\nwas 1074 (Monet), 584 (Cezanne), 401 (Van Gogh), 1433\\n(Ukiyo-e), and 6853 (Photographs). The Monet dataset was\\nparticularly pruned to include only landscape paintings, and\\nthe Van Gogh dataset included only his later works that rep-\\nresent his most recognizable artistic style.',\n",
       " 'Monet‚Äôs paintings‚Üíphotos To achieve high resolution\\nwhile conserving memory, we used random square crops\\nof the original images for training. To generate results, we\\npassed images of width 512 pixels with correct aspect ra-\\ntio to the generator network as input. The weight for the\\nidentity mapping loss was 0.5Œª where Œª was the weight for\\ncycle consistency loss. We set Œª = 10.',\n",
       " '\\x0c']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.868675Z",
     "start_time": "2020-12-19T15:06:46.719768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ —Ç—ã'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = translator.translate(\"Hi, how are you\", src='en', dest='ru')\n",
    "translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.884665Z",
     "start_time": "2020-12-19T15:06:46.874674Z"
    }
   },
   "outputs": [],
   "source": [
    "def formating_paragraph(text, word_in_line=7):\n",
    "    text_split_space = text.split(\" \")\n",
    "    pack = [\" \".join(text_split_space[i:i + word_in_line]) for i in range(0, len(text_split_space), word_in_line)]\n",
    "    return \"\\n\".join(pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:06:46.915650Z",
     "start_time": "2020-12-19T15:06:46.891665Z"
    }
   },
   "outputs": [],
   "source": [
    "def formating_paragraph_symbols(text, symbols_in_line=50):\n",
    "    pack = []\n",
    "    text_split_space = text.split(\" \")\n",
    "    count_symbols_in_line = 0\n",
    "    current_string = \"\"\n",
    "    for word in text_split_space:\n",
    "        if len(word) > symbols_in_line:\n",
    "            if current_string == \"\":\n",
    "                pack.append(word)\n",
    "            else:\n",
    "                pack.append(current_string)\n",
    "                pack.append(word)\n",
    "                count_symbols_in_line = 0\n",
    "                current_string = \"\"\n",
    "        elif count_symbols_in_line + len(word) > symbols_in_line:\n",
    "            pack.append(current_string)\n",
    "            count_symbols_in_line = len(word) + 1\n",
    "            current_string = word + \" \"\n",
    "        else:\n",
    "            count_symbols_in_line += len(word) + 1\n",
    "            current_string += (word + \" \")\n",
    "    if current_string != \"\":\n",
    "        pack.append(current_string)\n",
    "    return \"\\n\".join(pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:08:09.667814Z",
     "start_time": "2020-12-19T15:06:46.924643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n",
      "1 Jun-Yan Zhu‚àó\n",
      "2 Taesung Park‚àó\n",
      "3 Phillip Isola\n",
      "4 Alexei A. Efros\n",
      "5 Berkeley AI Research (BAIR) laboratory, UC Berkeley\n",
      "6 0 2 0 2   g u A   4 2     ]\n",
      "7 V C . s c [     7 v 3 9 5 0 1 . 3 0 7 1 : v i X r a\n",
      "8 Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically ‚Äútranslate‚Äù an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr. Example application (bottom): using a collection of paintings of famous artists, our method learns to render natural photographs into the respective styles.\n",
      "9 Abstract\n",
      "10 1. Introduction\n",
      "11 Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a train- ing set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X ‚Üí Y such that the distribution of images from G(X) is indistin- guishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we cou- ple it with an inverse mapping F : Y ‚Üí X and introduce a cycle consistency loss to enforce F (G(X)) ‚âà X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collec- tion style transfer, object transÔ¨Åguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.\n",
      "12 What did Claude Monet see as he placed his easel by the bank of the Seine near Argenteuil on a lovely spring day in 1873 (Figure 1, top-left)? A color photograph, had it been invented, may have documented a crisp blue sky and a glassy river reÔ¨Çecting it. Monet conveyed his impression of this same scene through wispy brush strokes and a bright palette.\n",
      "13 What if Monet had happened upon the little harbor in Cassis on a cool summer evening (Figure 1, bottom-left)? A brief stroll through a gallery of Monet paintings makes it possible to imagine how he would have rendered the scene: perhaps in pastel shades, with abrupt dabs of paint, and a somewhat Ô¨Çattened dynamic range.\n",
      "14 We can imagine all this despite never having seen a side by side example of a Monet painting next to a photo of the scene he painted. Instead, we have knowledge of the set of Monet paintings and of the set of landscape photographs. We can reason about the stylistic differences between these\n",
      "15 * indicates equal contribution\n",
      "16 1\n",
      "17 ZebrasHorseshorse        zebrazebra        horseSummer Wintersummer        winterwinter        summerPhotographVan GoghCezanneMonetUkiyo-eMonet        PhotosMonet        photophoto       Monet\f",
      "a mapping G : X ‚Üí Y such that the output ÀÜy = G(x), x ‚àà X, is indistinguishable from images y ‚àà Y by an ad- versary trained to classify ÀÜy apart from y. In theory, this ob- jective can induce an output distribution over ÀÜy that matches the empirical distribution pdata(y) (in general, this requires G to be stochastic) [16]. The optimal G thereby translates the domain X to a domain ÀÜY distributed identically to Y . However, such a translation does not guarantee that an in- dividual input x and output y are paired up in a meaningful way ‚Äì there are inÔ¨Ånitely many mappings G that will in- duce the same distribution over ÀÜy. Moreover, in practice, we have found it difÔ¨Åcult to optimize the adversarial objec- tive in isolation: standard procedures often lead to the well- known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15].\n",
      "18 These issues call for adding more structure to our ob- jective. Therefore, we exploit the property that translation should be ‚Äúcycle consistent‚Äù, in the sense that if we trans- late, e.g., a sentence from English to French, and then trans- late it back from French to English, we should arrive back at the original sentence [3]. Mathematically, if we have a translator G : X ‚Üí Y and another translator F : Y ‚Üí X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural as- sumption by training both the mapping G and F simultane- ously, and adding a cycle consistency loss [64] that encour- ages F (G(x)) ‚âà x and G(F (y)) ‚âà y. Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation.\n",
      "19 We apply our method to a wide range of applications, including collection style transfer, object transÔ¨Åguration, season transfer and photo enhancement. We also compare against previous approaches that rely either on hand-deÔ¨Åned factorizations of style and content, or on shared embed- ding functions, and show that our method outperforms these baselines. We provide both PyTorch and Torch implemen- tations. Check out more results at our website.\n",
      "20 2. Related work\n",
      "21 Generative Adversarial Networks (GANs) [16, 63] have achieved impressive results in image generation [6, 39], image editing [66], and representation learning [39, 43, 37]. Recent methods adopt the same idea for conditional image generation applications, such as text2image [41], im- age inpainting [38], and future prediction [36], as well as to other domains like videos [54] and 3D data [57]. The key to GANs‚Äô success is the idea of an adversarial loss that forces the generated images to be, in principle, indistinguishable from real photos. This loss is particularly powerful for im- age generation tasks, as this is exactly the objective that much of computer graphics aims to optimize. We adopt an adversarial loss to learn the mapping such that the translated\n",
      "22 Figure 2: Paired training data (left) consists of training ex- amples {xi, yi}N i=1, where the correspondence between xi and yi exists [22]. We instead consider unpaired training data (right), consisting of a source set {xi}N i=1 (xi ‚àà X) and a target set {yj}M j=1 (yj ‚àà Y ), with no information pro- vided as to which xi matches which yj.\n",
      "23 two sets, and thereby imagine what a scene might look like if we were to ‚Äútranslate‚Äù it from one set into the other.\n",
      "24 In this paper, we present a method that can learn to do the same: capturing special characteristics of one image col- lection and Ô¨Åguring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples.\n",
      "25 This problem can be more broadly described as image- to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph. Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs {xi, yi}N i=1 are avail- able (Figure 2, left), e.g., [11, 19, 22, 23, 28, 33, 45, 56, 58, 62]. However, obtaining paired training data can be difÔ¨Åcult and expensive. For example, only a couple of datasets ex- ist for tasks like semantic segmentation (e.g., [4]), and they are relatively small. Obtaining input-output pairs for graph- ics tasks like artistic stylization can be even more difÔ¨Åcult since the desired output is highly complex, typically requir- ing artistic authoring. For many tasks, like object transÔ¨Ågu- ration (e.g., zebra‚Üîhorse, Figure 1 top-middle), the desired output is not even well-deÔ¨Åned.\n",
      "26 We therefore seek an algorithm that can learn to trans- late between domains without paired input-output examples (Figure 2, right). We assume there is some underlying rela- tionship between the domains ‚Äì for example, that they are two different renderings of the same underlying scene ‚Äì and seek to learn that relationship. Although we lack supervi- sion in the form of paired examples, we can exploit super- vision at the level of sets: we are given one set of images in domain X and a different set in domain Y . We may train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ‚Ä¶‚Ä¶‚Ä¶PairedUnpaired\f",
      "Figure 3: (a) Our model contains two mapping functions G : X ‚Üí Y and F : Y ‚Üí X, and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX and F . To further regularize the mappings, we introduce two cycle consistency losses that capture the intuition that if we translate from one domain to the other and back again we should arrive at where we started: (b) forward cycle-consistency loss: x ‚Üí G(x) ‚Üí F (G(x)) ‚âà x, and (c) backward cycle-consistency loss: y ‚Üí F (y) ‚Üí G(F (y)) ‚âà y\n",
      "28 images cannot be distinguished from images in the target domain.\n",
      "29 Image-to-Image Translation The idea of image-to- image translation goes back at least to Hertzmann et al.‚Äôs Image Analogies [19], who employ a non-parametric tex- ture model [10] on a single input-output training image pair. More recent approaches use a dataset of input-output exam- ples to learn a parametric translation function using CNNs (e.g., [33]). Our approach builds on the ‚Äúpix2pix‚Äù frame- work of Isola et al. [22], which uses a conditional generative adversarial network [16] to learn a mapping from input to output images. Similar ideas have been applied to various tasks such as generating photographs from sketches [44] or from attribute and semantic layouts [25]. However, unlike the above prior work, we learn the mapping without paired training examples.\n",
      "30 Unpaired Image-to-Image Translation Several other methods also tackle the unpaired setting, where the goal is to relate two data domains: X and Y . Rosales et al. [42] propose a Bayesian framework that includes a prior based on a patch-based Markov random Ô¨Åeld computed from a source image and a likelihood term obtained from multiple style images. More recently, CoGAN [32] and cross-modal scene networks [1] use a weight-sharing strategy to learn a common representation across domains. Concurrent to our method, Liu et al. [31] extends the above framework with a combination of variational autoencoders [27] and genera- tive adversarial networks [16]. Another line of concurrent work [46, 49, 2] encourages the input and output to share speciÔ¨Åc ‚Äúcontent‚Äù features even though they may differ in ‚Äústyle‚Äú. These methods also use adversarial networks, with additional terms to enforce the output to be close to the input in a predeÔ¨Åned metric space, such as class label space [2], image pixel space [46], and image feature space [49].\n",
      "31 Unlike the above approaches, our formulation does not rely on any task-speciÔ¨Åc, predeÔ¨Åned similarity function be-\n",
      "32 tween the input and output, nor do we assume that the input and output have to lie in the same low-dimensional embed- ding space. This makes our method a general-purpose solu- tion for many vision and graphics tasks. We directly com- pare against several prior and contemporary approaches in Section 5.1.\n",
      "33 Cycle Consistency The idea of using transitivity as a way to regularize structured data has a long history. In visual tracking, enforcing simple forward-backward con- sistency has been a standard trick for decades [24, 48]. In the language domain, verifying and improving transla- tions via ‚Äúback translation and reconciliation‚Äù is a technique used by human translators [3] (including, humorously, by Mark Twain [51]), as well as by machines [17]. More recently, higher-order cycle consistency has been used in structure from motion [61], 3D shape matching [21], co- segmentation [55], dense semantic alignment [65, 64], and depth estimation [14]. Of these, Zhou et al. [64] and Go- dard et al. [14] are most similar to our work, as they use a cycle consistency loss as a way of using transitivity to su- pervise CNN training. In this work, we are introducing a similar loss to push G and F to be consistent with each other. Concurrent with our work, in these same proceed- ings, Yi et al. [59] independently use a similar objective for unpaired image-to-image translation, inspired by dual learning in machine translation [17].\n",
      "34 Neural Style Transfer [13, 23, 52, 12] is another way to perform image-to-image translation, which synthesizes a novel image by combining the content of one image with the style of another image (typically a painting) based on matching the Gram matrix statistics of pre-trained deep fea- tures. Our primary focus, on the other hand, is learning the mapping between two image collections, rather than be- tween two speciÔ¨Åc images, by trying to capture correspon- dences between higher-level appearance structures. There- fore, our method can be applied to other tasks, such as\n",
      "35 XYGFDYDXGFÀÜYXY(XY(GFÀÜX(a)(b)(c)cycle-consistencylosscycle-consistencylossDYDXÀÜyÀÜxxy\f",
      "painting‚Üí photo, object transÔ¨Åguration, etc. where single sample transfer methods do not perform well. We compare these two methods in Section 5.2.\n",
      "36 3. Formulation\n",
      "37 Our goal is to learn mapping functions between two domains X and Y given training samples {xi}N i=1 where j=1 where yj ‚àà Y 1. We denote the data xi ‚àà X and {yj}M distribution as x ‚àº pdata(x) and y ‚àº pdata(y). As illus- trated in Figure 3 (a), our model includes two mappings G : X ‚Üí Y and F : Y ‚Üí X. In addition, we in- troduce two adversarial discriminators DX and DY , where DX aims to distinguish between images {x} and translated images {F (y)}; in the same way, DY aims to discriminate between {y} and {G(x)}. Our objective contains two types of terms: adversarial losses [16] for matching the distribu- tion of generated images to the data distribution in the target domain; and cycle consistency losses to prevent the learned mappings G and F from contradicting each other.\n",
      "38 3.1. Adversarial Loss\n",
      "39 We apply adversarial losses [16] to both mapping func- tions. For the mapping function G : X ‚Üí Y and its dis- criminator DY , we express the objective as:\n",
      "40 LGAN(G, DY , X, Y ) = E + E\n",
      "41 y‚àºpdata(y)[log DY (y)] x‚àºpdata(x)[log(1 ‚àí DY (G(x))],\n",
      "42 (1)\n",
      "43 where G tries to generate images G(x) that look similar to images from domain Y , while DY aims to distinguish be- tween translated samples G(x) and real samples y. G aims to minimize this objective against an adversary D that tries i.e., minG maxDY LGAN(G, DY , X, Y ). to maximize it, We introduce a similar adversarial loss for the mapping function F : Y ‚Üí X and its discriminator DX as well: i.e., minF maxDX LGAN(F, DX , Y, X).\n",
      "44 3.2. Cycle Consistency Loss\n",
      "45 Adversarial training can, in theory, learn mappings G and F that produce outputs identically distributed as target domains Y and X respectively (strictly speaking, this re- quires G and F to be stochastic functions) [15]. However, with large enough capacity, a network can map the same set of input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target dis- tribution. Thus, adversarial losses alone cannot guarantee that the learned function can map an individual input xi to a desired output yi. To further reduce the space of possi- ble mapping functions, we argue that the learned mapping\n",
      "46 1We often omit the subscript i and j for simplicity.\n",
      "47 Figure 4: The input images x, output images G(x) and the reconstructed images F (G(x)) from various experiments. From top to bottom: photo‚ÜîCezanne, horses‚Üîzebras, winter‚Üísummer Yosemite, aerial photos‚ÜîGoogle maps.\n",
      "48 functions should be cycle-consistent: as shown in Figure 3 (b), for each image x from domain X, the image translation cycle should be able to bring x back to the original image, i.e., x ‚Üí G(x) ‚Üí F (G(x)) ‚âà x. We call this forward cy- cle consistency. Similarly, as illustrated in Figure 3 (c), for each image y from domain Y , G and F should also satisfy backward cycle consistency: y ‚Üí F (y) ‚Üí G(F (y)) ‚âà y. We incentivize this behavior using a cycle consistency loss:\n",
      "49 Lcyc(G, F ) = E + E\n",
      "50 x‚àºpdata(x)[(cid:107)F (G(x)) ‚àí x(cid:107)1] y‚àºpdata(y)[(cid:107)G(F (y)) ‚àí y(cid:107)1].\n",
      "51 (2)\n",
      "52 In preliminary experiments, we also tried replacing the L1 norm in this loss with an adversarial loss between F (G(x)) and x, and between G(F (y)) and y, but did not observe improved performance.\n",
      "53 The behavior induced by the cycle consistency loss can be observed in Figure 4: the reconstructed images F (G(x)) end up matching closely to the input images x.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 3.3. Full Objective\n",
      "55 Our full objective is:\n",
      "56 L(G, F, DX , DY ) =LGAN(G, DY , X, Y ) + LGAN(F, DX , Y, X) + ŒªLcyc(G, F ),\n",
      "57 (3)\n",
      "58 Input ùë•Outputùê∫(ùë•)ReconstructionF(ùê∫ùë•)\f",
      "where Œª controls the relative importance of the two objec- tives. We aim to solve:\n",
      "59 G‚àó, F ‚àó = arg min G,F\n",
      "60 max Dx,DY\n",
      "61 L(G, F, DX , DY ).\n",
      "62 (4)\n",
      "63 Notice that our model can be viewed as training two ‚Äúau- toencoders‚Äù [20]: we learn one autoencoder F ‚ó¶ G : X ‚Üí X jointly with another G ‚ó¶ F : Y ‚Üí Y . However, these au- toencoders each have special internal structures: they map an image to itself via an intermediate representation that is a translation of the image into another domain. Such a setup can also be seen as a special case of ‚Äúadversarial au- toencoders‚Äù [34], which use an adversarial loss to train the bottleneck layer of an autoencoder to match an arbitrary tar- get distribution. In our case, the target distribution for the X ‚Üí X autoencoder is that of the domain Y .\n",
      "64 In Section 5.1.4, we compare our method against ab- lations of the full objective, including the adversarial loss LGAN alone and the cycle consistency loss Lcyc alone, and empirically show that both objectives play critical roles in arriving at high-quality results. We also evaluate our method with only cycle loss in one direction and show that a single cycle is not sufÔ¨Åcient to regularize the training for this under-constrained problem.\n",
      "65 4. Implementation\n",
      "66 Network Architecture We adopt the architecture for our generative networks from Johnson et al. [23] who have shown impressive results for neural style transfer and super- resolution. This network contains three convolutions, sev- eral residual blocks [18], two fractionally-strided convo- lutions with stride 1 2 , and one convolution that maps fea- tures to RGB. We use 6 blocks for 128 √ó 128 images and 9 blocks for 256 √ó 256 and higher-resolution training images. Similar to Johnson et al. [23], we use instance normaliza- tion [53]. For the discriminator networks we use 70 √ó 70 PatchGANs [22, 30, 29], which aim to classify whether 70 √ó 70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily- sized images in a fully convolutional fashion [22].\n",
      "67 Training details We apply two techniques from recent works to stabilize our model training procedure. First, for LGAN (Equation 1), we replace the negative log like- lihood objective by a least-squares loss [35]. This loss is more stable during training and generates higher quality results. In particular, for a GAN loss LGAN(G, D, X, Y ), we train the G to minimize E x‚àºpdata(x)[(D(G(x)) ‚àí 1)2] and train the D to minimize E y‚àºpdata(y)[(D(y) ‚àí 1)2] + E\n",
      "68 x‚àºpdata(x)[D(G(x))2]. Second, to reduce model oscillation [15], we follow Shrivastava et al.‚Äôs strategy [46] and update the discrimi-\n",
      "69 nators using a history of generated images rather than the ones produced by the latest generators. We keep an image buffer that stores the 50 previously created images.\n",
      "70 For all the experiments, we set Œª = 10 in Equation 3. We use the Adam solver [26] with a batch size of 1. All networks were trained from scratch with a learning rate of 0.0002. We keep the same learning rate for the Ô¨Årst 100 epochs and linearly decay the rate to zero over the next 100 epochs. Please see the appendix (Section 7) for more details about the datasets, architectures, and training procedures.\n",
      "72 We Ô¨Årst compare our approach against recent methods for unpaired image-to-image translation on paired datasets where ground truth input-output pairs are available for eval- uation. We then study the importance of both the adversar- ial loss and the cycle consistency loss and compare our full method against several variants. Finally, we demonstrate the generality of our algorithm on a wide range of applica- tions where paired data does not exist. For brevity, we refer to our method as CycleGAN. The PyTorch and Torch code, models, and full results can be found at our website.\n",
      "74 Using the same evaluation datasets and metrics as ‚Äúpix2pix‚Äù [22], we compare our method against several baselines both qualitatively and quantitatively. The tasks in- clude semantic labels‚Üîphoto on the Cityscapes dataset [4], and map‚Üîaerial photo on data scraped from Google Maps. We also perform ablation study on the full loss function.\n",
      "76 AMT perceptual studies On the map‚Üîaerial photo task, we run ‚Äúreal vs fake‚Äù perceptual studies on Amazon Mechanical Turk (AMT) to assess the realism of our out- puts. We follow the same perceptual study protocol from Isola et al. [22], except we only gather data from 25 partic- ipants per algorithm we tested. Participants were shown a sequence of pairs of images, one a real photo or map and one fake (generated by our algorithm or a baseline), and asked to click on the image they thought was real. The Ô¨Årst 10 trials of each session were practice and feedback was given as to whether the participant‚Äôs response was correct or incorrect. The remaining 40 trials were used to assess the rate at which each algorithm fooled participants. Each session only tested a single algorithm, and participants were only allowed to complete a single session. The numbers we report here are not directly comparable to those in [22] as our ground truth images were processed slightly differently 2 and the participant pool we tested may be differently dis-\n",
      "77 2We train all the models on 256 √ó 256 images while in pix2pix [22], the model was trained on 256 √ó 256 patches of 512 √ó 512 images, and\n",
      "78 \f",
      "Figure 5: Different methods for mapping labels‚Üîphotos trained on Cityscapes images. From left to right: input, Bi- GAN/ALI [7, 9], CoGAN [32], feature loss + GAN, SimGAN [46], CycleGAN (ours), pix2pix [22] trained on paired data, and ground truth.\n",
      "79 Figure 6: Different methods for mapping aerial photos‚Üîmaps on Google Maps. From left to right: input, BiGAN/ALI [7, 9], CoGAN [32], feature loss + GAN, SimGAN [46], CycleGAN (ours), pix2pix [22] trained on paired data, and ground truth.\n",
      "80 tributed from those tested in [22] (due to running the exper- iment at a different date and time). Therefore, our numbers should only be used to compare our current method against the baselines (which were run under identical conditions), rather than against [22].\n",
      "81 FCN score Although perceptual studies may be the gold standard for assessing graphical realism, we also seek an automatic quantitative measure that does not require human experiments. For this, we adopt the ‚ÄúFCN score‚Äù from [22], and use it to evaluate the Cityscapes labels‚Üíphoto task. The FCN metric evaluates how interpretable the generated photos are according to an off-the-shelf semantic segmen- tation algorithm (the fully-convolutional network, FCN, from [33]). The FCN predicts a label map for a generated photo. This label map can then be compared against the input ground truth labels using standard semantic segmen-\n",
      "82 run convolutionally on the 512 √ó 512 images at test time. We choose 256 √ó 256 in our experiments as many baselines cannot scale up to high- resolution images, and CoGAN cannot be tested fully convolutionally.\n",
      "83 tation metrics described below. The intuition is that if we generate a photo from a label map of ‚Äúcar on the road‚Äù, then we have succeeded if the FCN applied to the generated photo detects ‚Äúcar on the road‚Äù.\n",
      "84 Semantic segmentation metrics To evaluate the perfor- mance of photo‚Üílabels, we use the standard metrics from the Cityscapes benchmark [4], including per-pixel accuracy, per-class accuracy, and mean class Intersection-Over-Union (Class IOU) [4].\n",
      "86 CoGAN [32] This method learns one GAN generator for domain X and one for domain Y , with tied weights on the Ô¨Årst few layers for shared latent representations. Translation from X to Y can be achieved by Ô¨Ånding a latent represen- tation that generates image X and then rendering this latent representation into style Y .\n",
      "87 SimGAN [46] Like our method, Shrivastava et al.[46] uses an adversarial loss to train a translation from X to Y .\n",
      "88 InputBiGANCoGANfeature loss GANSimGANCycleGANpix2pixGround truthInputBiGANCoGANfeature loss GANSimGANCycleGANpix2pixGround truth\f",
      "Table 1: AMT ‚Äúreal vs fake‚Äù test on maps‚Üîaerial photos at 256 √ó 256 resolution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 Table 4: Ablation study: FCN-scores for different variants of our method, evaluated on Cityscapes labels‚Üíphoto.\n",
      "90 Per-pixel acc. Per-class acc. Class IOU\n",
      "91 Per-pixel acc. Per-class acc. Class IOU\n",
      "92 Loss CoGAN [32] BiGAN/ALI [9, 7] SimGAN [46] Feature loss + GAN CycleGAN (ours)\n",
      "93 Map ‚Üí Photo\n",
      "94 Photo ‚Üí Map\n",
      "95 % Turkers labeled real % Turkers labeled real\n",
      "96 0.6% ¬± 0.5% 2.1% ¬± 1.0% 0.7% ¬± 0.5% 1.2% ¬± 0.6% 26.8% ¬± 2.8%\n",
      "97 0.9% ¬± 0.5% 1.9% ¬± 0.9% 2.6% ¬± 1.1% 0.3% ¬± 0.2% 23.2% ¬± 3.4%\n",
      "98 Loss CoGAN [32] BiGAN/ALI [9, 7] SimGAN [46] Feature loss + GAN CycleGAN (ours) pix2pix [22]\n",
      "99 Loss CoGAN [32] BiGAN/ALI [9, 7] SimGAN [46] Feature loss + GAN CycleGAN (ours) pix2pix [22]\n",
      "100 0.40 0.19 0.20 0.06 0.52 0.71\n",
      "101 0.45 0.41 0.47 0.50 0.58 0.85\n",
      "102 0.10 0.06 0.10 0.04 0.17 0.25\n",
      "103 0.11 0.13 0.11 0.10 0.22 0.40\n",
      "104 0.06 0.02 0.04 0.01 0.11 0.18\n",
      "105 0.08 0.07 0.07 0.06 0.16 0.32\n",
      "106 Table 2: FCN-scores for different methods, evaluated on Cityscapes labels‚Üíphoto.\n",
      "107 Per-pixel acc. Per-class acc. Class IOU\n",
      "108 Table 3: ClassiÔ¨Åcation performance of photo‚Üílabels for different methods on cityscapes.\n",
      "109 The regularization term (cid:107)x ‚àí G(x)(cid:107)1 i s used to penalize making large changes at pixel level.\n",
      "110 Feature loss + GAN We also test a variant of Sim- GAN [46] where the L1 loss is computed over deep image features using a pretrained network (VGG-16 relu4 2 [47]), rather than over RGB pixel values. Com- puting distances in deep feature space, like this, is also sometimes referred to as using a ‚Äúperceptual loss‚Äù [8, 23].\n",
      "111 BiGAN/ALI [9, 7] Unconditional GANs [16] learn a generator G : Z ‚Üí X, that maps a random noise z to an image x. The BiGAN [9] and ALI [7] propose to also learn the inverse mapping function F : X ‚Üí Z. Though they were originally designed for mapping a latent vector z to an image x, we implemented the same objective for mapping a source image x to a target image y.\n",
      "112 pix2pix [22] We also compare against pix2pix [22], which is trained on paired data, to see how close we can get to this ‚Äúupper bound‚Äù without using any paired data.\n",
      "113 For a fair comparison, we implement all the baselines using the same architecture and details as our method, ex- cept for CoGAN [32]. CoGAN builds on generators that produce images from a shared latent representation, which is incompatible with our image-to-image network. We use the public implementation of CoGAN instead.\n",
      "115 As can be seen in Figure 5 and Figure 6, we were unable to achieve compelling results with any of the baselines. Our\n",
      "116 Per-pixel acc. Per-class acc. Class IOU\n",
      "117 Loss Cycle alone GAN alone GAN + forward cycle GAN + backward cycle CycleGAN (ours)\n",
      "118 Loss Cycle alone GAN alone GAN + forward cycle GAN + backward cycle CycleGAN (ours)\n",
      "119 0.22 0.51 0.55 0.39 0.52\n",
      "120 0.10 0.53 0.49 0.01 0.58\n",
      "121 0.07 0.11 0.18 0.14 0.17\n",
      "122 0.05 0.11 0.11 0.06 0.22\n",
      "123 0.02 0.08 0.12 0.06 0.11\n",
      "124 0.02 0.07 0.07 0.01 0.16\n",
      "125 Table 5: Ablation study: classiÔ¨Åcation performance of photo‚Üílabels for different losses, evaluated on Cityscapes.\n",
      "126 method, on the other hand, can produce translations that are often of similar quality to the fully supervised pix2pix.\n",
      "127 Table 1 reports performance regarding the AMT per- ceptual realism task. Here, we see that our method can fool participants on around a quarter of trials, in both the maps‚Üíaerial photos direction and the aerial photos‚Üímaps direction at 256 √ó 256 resolution3. All the baselines almost never fooled participants.\n",
      "128 Table 2 assesses the performance of the labels‚Üíphoto task on the Cityscapes and Table 3 evaluates the opposite mapping (photos‚Üílabels). In both cases, our method again outperforms the baselines.\n",
      "130 In Table 4 and Table 5, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cy- cle loss in only one direction: GAN + forward cycle loss E x‚àºpdata(x)[(cid:107)F (G(x))‚àíx(cid:107)1], or GAN + backward cycle loss E y‚àºpdata(y)[(cid:107)G(F (y))‚àíy(cid:107)1] (Equation 2) and Ô¨Ånd that it of- ten incurs training instability and causes mode collapse, es- pecially for the direction of the mapping that was removed. Figure 7 shows several qualitative examples.\n",
      "132 Image reconstruction quality\n",
      "133 In Figure 4, we show a few random samples of the recon- structed images F (G(x)). We observed that the recon- structed images were often close to the original inputs x, at both training and testing time, even in cases where one domain represents signiÔ¨Åcantly more diverse information, such as map‚Üîaerial photos.\n",
      "134 3We also train CycleGAN and pix2pix at 512 √ó 512 resolution, and observe the comparable performance: maps‚Üíaerial photos: CycleGAN: 37.5% ¬± 3.6% and pix2pix: 33.9% ¬± 3.1%; aerial photos‚Üímaps: Cy- cleGAN: 16.5% ¬± 4.1% and pix2pix: 8.5% ¬± 2.6%\n",
      "135 \f",
      "Figure 7: Different variants of our method for mapping labels‚Üîphotos trained on cityscapes. From left to right: input, cycle- consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss (F (G(x)) ‚âà x), GAN + backward cycle-consistency loss (G(F (y)) ‚âà y), CycleGAN (our full method), and ground truth. Both Cycle alone and GAN + backward fail to produce images similar to the target domain. GAN alone and GAN + forward suffer from mode collapse, producing identical label maps regardless of the input photo.\n",
      "136 the appendix (Section 7) for more details about the datasets. We observe that translations on training data are often more appealing than those on test data, and full results of all ap- plications on both training and test data can be viewed on our project website.\n",
      "137 Collection style transfer (Figure 10 and Figure 11) We train the model on landscape photographs downloaded from Flickr and WikiArt. Unlike recent work on ‚Äúneural style transfer‚Äù [13], our method learns to mimic the style of an entire collection of artworks, rather than transferring the style of a single selected piece of art. Therefore, we can learn to generate photos in the style of, e.g., Van Gogh, rather than just in the style of Starry Night. The size of the dataset for each artist/style was 526, 1073, 400, and 563 for Cezanne, Monet, Van Gogh, and Ukiyo-e.\n",
      "138 Object transÔ¨Åguration (Figure 13) The model is trained to translate one object class from ImageNet [5] to another (each class contains around 1000 training images). Turmukhambetov et al. [50] propose a subspace model to translate one object into another object of the same category, while our method focuses on object transÔ¨Åguration between two visually similar categories.\n",
      "139 Season transfer (Figure 13) The model is trained on 854 winter photos and 1273 summer photos of Yosemite downloaded from Flickr.\n",
      "140 Photo generation from paintings (Figure 12) For painting‚Üíphoto, we Ô¨Ånd that it is helpful to introduce an additional loss to encourage the mapping to preserve color composition between the input and output. In particular, we adopt the technique of Taigman et al. [49] and regularize the generator to be near an identity mapping when real samples of the target domain are provided as the input to the gen- erator: i.e., Lidentity(G, F ) = E y‚àºpdata(y)[(cid:107)G(y) ‚àí y(cid:107)1] + E\n",
      "141 x‚àºpdata(x)[(cid:107)F (x) ‚àí x(cid:107)1].\n",
      "142 Figure 8: Example results of CycleGAN on paired datasets used in ‚Äúpix2pix‚Äù [22] such as architectural labels‚Üîphotos and edges‚Üîshoes.\n",
      "144 Figure 8 shows some example results on other paired datasets used in ‚Äúpix2pix‚Äù [22], such as architectural labels‚Üîphotos from the CMP Facade Database [40], and edges‚Üîshoes from the UT Zappos50K dataset [60]. The image quality of our results is close to those produced by the fully supervised pix2pix while our method learns the mapping without paired supervision.\n",
      "146 We demonstrate our method on several applications where paired training data does not exist. Please refer to\n",
      "147 Ground truthInputGAN aloneCycle aloneGAN+forwardGAN+backwardCycleGANlabel ‚Üífacadefacade ‚Üílabeledges  ‚Üíshoesshoes  ‚ÜíedgesInputOutputInputOutputInputOutput\f",
      "collection, we compute the average Gram Matrix across the target domain and use this matrix to transfer the ‚Äúaverage style‚Äù with Gatys et al [13].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 Figure 16 demonstrates similar comparisons for other translation tasks. We observe that Gatys et al. [13] requires Ô¨Ånding target style images that closely match the desired output, but still often fails to produce photorealistic results, while our method succeeds to generate natural-looking re- sults, similar to the target domain.\n",
      "149 6. Limitations and Discussion\n",
      "150 Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Fig- ure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with lit- tle success. For example, on the task of dog‚Üícat transÔ¨Ågu- ration, the learned translation degenerates into making min- imal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geo- metric changes, is an important problem for future work.\n",
      "151 Some failure cases are caused by the distribution charac- teristics of the training datasets. For example, our method has got confused in the horse ‚Üí zebra example (Figure 17, right), because our model was trained on the wild horse and zebra synsets of ImageNet, which does not contain images of a person riding a horse or zebra.\n",
      "152 We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very for example, our hard ‚Äì or even impossible ‚Äì to close: method sometimes permutes the labels for tree and build- ing in the output of the photos‚Üílabels task. Resolving this ambiguity may require some form of weak semantic super- vision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.\n",
      "153 Nonetheless, in many cases completely unpaired data is plentifully available and should be made use of. This paper pushes the boundaries of what is possible in this ‚Äúunsuper- vised‚Äù setting.\n",
      "154 Acknowledgments: We thank Aaron Hertzmann, Shiry Ginosar, Deepak Pathak, Bryan Russell, Eli Shechtman, Richard Zhang, and Tinghui Zhou for many helpful com- ments. This work was supported in part by NSF SMA- 1514512, NSF IIS-1633310, a Google Research Award, In- tel Corp, and hardware donations from NVIDIA. JYZ is supported by the Facebook Graduate Fellowship and TP is supported by the Samsung Scholarship. The photographs used for style transfer were taken by AE, mostly in France.\n",
      "155 Figure 9: The effect of the identity mapping loss on Monet‚Äôs painting‚Üí photos. From left to right: input paintings, Cy- cleGAN without identity mapping loss, CycleGAN with identity mapping loss. The identity mapping loss helps pre- serve the color of the input paintings.\n",
      "156 Without Lidentity,\n",
      "157 the generator G and F are free to change the tint of input images when there is no need to. For example, when learning the mapping between Monet‚Äôs paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the ad- versarial loss and cycle consistency loss. The effect of this identity mapping loss are shown in Figure 9.\n",
      "158 In Figure 12, we show additional results translating Monet‚Äôs paintings to photographs. This Ô¨Ågure and Figure 9 show results on paintings that were included in the train- ing set, whereas for all other experiments in the paper, we only evaluate and show test set results. Because the training set does not include paired data, coming up with a plausi- ble translation for a training set painting is a nontrivial task. Indeed, since Monet is no longer able to create new paint- ings, generalization to unseen, ‚Äútest set‚Äù, paintings is not a pressing problem.\n",
      "159 Photo enhancement (Figure 14) We show that our method can be used to generate photos with shallower depth of Ô¨Åeld. We train the model on Ô¨Çower photos downloaded from Flickr. The source domain consists of Ô¨Çower photos taken by smartphones, which usually have deep DoF due to a small aperture. The target contains photos captured by DSLRs with a larger aperture. Our model successfully gen- erates photos with shallower depth of Ô¨Åeld from the photos taken by smartphones.\n",
      "160 Comparison with Gatys et al. [13] In Figure 15, we compare our results with neural style transfer [13] on photo stylization. For each row, we Ô¨Årst use two representative artworks as the style images for [13]. Our method, on the other hand, can produce photos in the style of entire collec- tion. To compare against neural style transfer of an entire\n",
      "161 CycleGANInputCycleGAN+L\"#$%&\"&‚Äô\f",
      "Figure 10: Collection style transfer I: we transfer input images into the artistic styles of Monet, Van Gogh, Cezanne, and Ukiyo-e. Please see our website for additional examples.\n",
      "162 Ukiyo-eMonetInputVan GoghCezanne\f",
      "Figure 11: Collection style transfer II: we transfer input images into the artistic styles of Monet, Van Gogh, Cezanne, Ukiyo-e. Please see our website for additional examples.\n",
      "163 MonetUkiyo-eInputVan GoghCezanne\f",
      "Figure 12: Relatively successful results on mapping Monet‚Äôs paintings to a photographic style. Please see our website for additional examples.\n",
      "164 InputOutputInputOutput\f",
      "Figure 13: Our method applied to several translation problems. These images are selected as relatively successful results ‚Äì please see our website for more comprehensive and random results. In the top two rows, we show results on object transÔ¨Åguration between horses and zebras, trained on 939 images from the wild horse class and 1177 images from the zebra class in Imagenet [5]. Also check out the horse‚Üízebra demo video. The middle two rows show results on season transfer, trained on winter and summer photos of Yosemite from Flickr. In the bottom two rows, we train our method on 996 apple images and 1020 navel orange images from ImageNet.\n",
      "165 InputInputInputOutputOutputOutputhorse ‚Üízebrazebra ‚Üíhorsesummer Yosemite ‚Üíwinter Yosemite apple ‚Üíorangeorange ‚Üíapplewinter Yosemite ‚Üísummer Yosemite\f",
      "Figure 14: Photo enhancement: mapping from a set of smartphone snaps to professional DSLR photographs, the system often learns to produce shallow focus. Here we show some of the most successful results in our test set ‚Äì average performance is considerably worse. Please see our website for more comprehensive and random examples.\n",
      "166 Figure 15: We compare our method with neural style transfer [13] on photo stylization. Left to right: input image, results from Gatys et al. [13] using two different representative artworks as style images, results from Gatys et al. [13] using the entire collection of the artist, and CycleGAN (ours).\n",
      "167 InputOutputInputOutputInputOutputInputOutputInputGatyset al. (image I)CycleGANGatyset al. (image II)Gatyset al. (collection)Photo ‚ÜíVan Gogh Photo ‚ÜíUkiyo-ePhoto ‚ÜíCezanne\f",
      "Figure 16: We compare our method with neural style transfer [13] on various applications. From top to bottom: apple‚Üíorange, horse‚Üízebra, and Monet‚Üíphoto. Left to right: input image, results from Gatys et al. [13] using two different images as style images, results from Gatys et al. [13] using all the images from the target domain, and CycleGAN (ours).\n",
      "168 Figure 17: Typical failure cases of our method. Left: in the task of dog‚Üícat transÔ¨Åguration, CycleGAN can only make minimal changes to the input. Right: CycleGAN also fails in this horse ‚Üí zebra example as our model has not seen images of horseback riding during training. Please see our website for more comprehensive results.\n",
      "169 InputGatyset al. (image I)CycleGANGatyset al. (image II)Gatyset al. (collection)apple ‚Üíorangehorse  ‚ÜízebraMonet ‚ÜíphotoInputOutputInputOutputapple ‚Üíorangezebra ‚Üíhorsedog ‚Üícatcat ‚Üídogwinter ‚ÜísummerMonet ‚Üíphotophoto ‚ÜíUkiyo-ephoto ‚ÜíVan GoghInputOutputiPhone photo ‚ÜíDSLR photohorse‚ÜízebraImageNet‚Äúwildhorse‚ÄùtrainingimagesInputOutput\f",
      "References\n",
      "170 [1] Y. Aytar, L. Castrejon, C. Vondrick, H. Pirsiavash, and A. Torralba. Cross-modal scene networks. PAMI, 2016. 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 [2] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adap- tation with generative adversarial networks. In CVPR, 2017. 3\n",
      "172 [3] R. W. Brislin. Back-translation for cross-cultural Journal of cross-cultural psychology,\n",
      "173 research. 1(3):185‚Äì216, 1970. 2, 3\n",
      "174 [4] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. En- zweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 2, 5, 6, 18 [5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical im- age database. In CVPR, 2009. 8, 13, 18\n",
      "175 [6] E. L. Denton, S. Chintala, R. Fergus, et al. Deep gen- erative image models using a laplacian pyramid of ad- versarial networks. In NIPS, 2015. 2\n",
      "176 [7] J. Donahue, P. Kr¬®ahenb¬®uhl, and T. Darrell. Adversarial\n",
      "177 feature learning. In ICLR, 2017. 6, 7\n",
      "178 [8] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on deep networks. In NIPS, 2016. 7\n",
      "179 [9] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Ar- jovsky, O. Mastropietro, and A. Courville. Adversari- ally learned inference. In ICLR, 2017. 6, 7\n",
      "180 non-parametric sampling. In ICCV, 1999. 3\n",
      "181 [11] D. Eigen and R. Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale convolutional architecture. In ICCV, 2015. 2\n",
      "182 [12] L. A. Gatys, M. Bethge, A. Hertzmann, and E. Shecht- man. Preserving color in neural artistic style transfer. arXiv preprint arXiv:1606.05897, 2016. 3\n",
      "183 [13] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. CVPR, 2016. 3, 8, 9, 14, 15\n",
      "184 [14] C. Godard, O. Mac Aodha, and G. J. Brostow. Un- supervised monocular depth estimation with left-right consistency. In CVPR, 2017. 3\n",
      "185 [15] I. Goodfellow. NIPS 2016 tutorial: Generative ad- versarial networks. arXiv preprint arXiv:1701.00160, 2016. 2, 4, 5\n",
      "186 [16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben- gio. Generative adversarial nets. In NIPS, 2014. 2, 3, 4, 7\n",
      "187 [17] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In NIPS, 2016. 3\n",
      "188 [18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 5\n",
      "189 [19] A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H. Salesin. Image analogies. In SIGGRAPH, 2001. 2, 3\n",
      "190 [20] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504‚Äì507, 2006. 5\n",
      "191 [21] Q.-X. Huang and L. Guibas. Consistent shape maps via semideÔ¨Ånite programming. In Symposium on Ge- ometry Processing, 2013. 3\n",
      "192 [22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image- to-image translation with conditional adversarial net- works. In CVPR, 2017. 2, 3, 5, 6, 7, 8, 18\n",
      "193 [23] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 2, 3, 5, 7, 18\n",
      "194 [24] Z. Kalal, K. Mikolajczyk, and J. Matas. Forward- backward error: Automatic detection of tracking fail- ures. In ICPR, 2010. 3\n",
      "195 [25] L. Karacan, Z. Akata, A. Erdem, and E. Erdem. Learning to generate images of outdoor scenes from arXiv preprint attributes and semantic layouts. arXiv:1612.00215, 2016. 3\n",
      "196 [26] D. Kingma and J. Ba. Adam: A method for stochastic\n",
      "197 [27] D. P. Kingma and M. Welling. Auto-encoding varia-\n",
      "198 tional bayes. ICLR, 2014. 3\n",
      "199 [28] P.-Y. Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays. Transient attributes for high-level understanding and editing of outdoor scenes. ACM TOG, 33(4):149, 2014. 2\n",
      "200 [29] C. Ledig, L. Theis, F. Husz¬¥ar, J. Caballero, A. Cun- ningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al. Photo-realistic single image super- resolution using a generative adversarial network. In CVPR, 2017. 5\n",
      "201 [30] C. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial net- works. ECCV, 2016. 5\n",
      "202 [31] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. In NIPS, 2017. 3\n",
      "203 [32] M.-Y. Liu and O. Tuzel. Coupled generative adversar-\n",
      "204 ial networks. In NIPS, 2016. 3, 6, 7\n",
      "205 [10] A. A. Efros and T. K. Leung. Texture synthesis by\n",
      "206 optimization. In ICLR, 2015. 5\n",
      "207 \f",
      "[33] J. Long, E. Shelhamer, and T. Darrell. Fully convolu- tional networks for semantic segmentation. In CVPR, 2015. 2, 3, 6\n",
      "208 [34] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. In ICLR, 2016. 5 [35] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial net- works. In CVPR. IEEE, 2017. 5\n",
      "209 [36] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi- scale video prediction beyond mean square error. In ICLR, 2016. 2\n",
      "210 [37] M. F. Mathieu, J. Zhao, A. Ramesh, P. Sprechmann, and Y. LeCun. Disentangling factors of variation in deep representation using adversarial training. In NIPS, 2016. 2\n",
      "211 [38] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. CVPR, 2016. 2\n",
      "212 [39] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional gen- erative adversarial networks. In ICLR, 2016. 2 [40] R. ÀáS. Radim TyleÀácek. Spatial pattern templates for recognition of objects with regular structure. In Proc. GCPR, Saarbrucken, Germany, 2013. 8, 18\n",
      "213 [41] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image syn- thesis. In ICML, 2016. 2\n",
      "214 [42] R. Rosales, K. Achan, and B. J. Frey. Unsupervised\n",
      "215 image translation. In ICCV, 2003. 3\n",
      "216 [43] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, Improved techniques for\n",
      "217 A. Radford, and X. Chen. training GANs. In NIPS, 2016. 2\n",
      "218 [44] P. Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays. Scrib- bler: Controlling deep image synthesis with sketch and color. In CVPR, 2017. 3\n",
      "219 [45] Y. Shih, S. Paris, F. Durand, and W. T. Freeman. Data- driven hallucination of different times of day from a single outdoor photo. ACM TOG, 32(6):200, 2013. 2 [46] A. Shrivastava, T. PÔ¨Åster, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. In CVPR, 2017. 3, 5, 6, 7\n",
      "220 [47] K. Simonyan and A. Zisserman. Very deep convolu- tional networks for large-scale image recognition. In ICLR, 2015. 7\n",
      "221 [48] N. Sundaram, T. Brox, and K. Keutzer. Dense point trajectories by gpu-accelerated large displacement op- tical Ô¨Çow. In ECCV, 2010. 3\n",
      "222 [49] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised\n",
      "223 cross-domain image generation. In ICLR, 2017. 3, 8\n",
      "224 [50] D. Turmukhambetov, N. D. Campbell, S. J. Prince, and J. Kautz. Modeling object appearance using In CVPR, context-conditioned component analysis. 2015. 8\n",
      "225 [51] M. Twain. The jumping frog:\n",
      "226 in english, then in french, and then clawed back into a civilized language once more by patient. Unremunerated Toil, 3, 1903. 3 [52] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempit- sky. Texture networks: Feed-forward synthesis of tex- tures and stylized images. In ICML, 2016. 3\n",
      "227 [53] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast styliza- tion. arXiv preprint arXiv:1607.08022, 2016. 5 [54] C. Vondrick, H. Pirsiavash, and A. Torralba. Generat- ing videos with scene dynamics. In NIPS, 2016. 2\n",
      "228 [55] F. Wang, Q. Huang, and L. J. Guibas.\n",
      "229 Image co- segmentation via consistent functional maps. In ICCV, 2013. 3\n",
      "230 [56] X. Wang and A. Gupta. Generative image model- ing using style and structure adversarial networks. In ECCV, 2016. 2\n",
      "231 [57] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenen- baum. Learning a probabilistic latent space of ob- ject shapes via 3d generative-adversarial modeling. In NIPS, 2016. 2\n",
      "232 [58] S. Xie and Z. Tu. Holistically-nested edge detection.\n",
      "233 In ICCV, 2015. 2\n",
      "234 [59] Z. Yi, H. Zhang, T. Gong, Tan, and M. Gong. Dual- gan: Unsupervised dual learning for image-to-image translation. In ICCV, 2017. 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 [60] A. Yu and K. Grauman. Fine-grained visual compar- isons with local learning. In CVPR, 2014. 8, 18 [61] C. Zach, M. Klopschitz, and M. Pollefeys. Disam- biguating visual relations using loop constraints. In CVPR, 2010. 3\n",
      "236 [62] R. Zhang, P. Isola, and A. A. Efros. Colorful image\n",
      "237 colorization. In ECCV, 2016. 2\n",
      "238 [63] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2017. 2 [64] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros. Learning dense correspondence via 3d- guided cycle consistency. In CVPR, 2016. 2, 3 [65] T. Zhou, Y. J. Lee, S. Yu, and A. A. Efros. Flowweb: image set alignment by weaving consistent,\n",
      "239 Joint pixel-wise correspondences. In CVPR, 2015. 3 [66] J.-Y. Zhu, P. Kr¬®ahenb¬®uhl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In ECCV, 2016. 2\n",
      "240 \f",
      "Flower photo enhancement Flower images taken on smartphones were downloaded from Flickr by searching for the photos taken by Apple iPhone 5, 5s, or 6, with search text Ô¨Çower. DSLR images with shallow DoF were also downloaded from Flickr by search tag Ô¨Çower, dof. The im- ages were scaled to 360 pixels by width. The identity map- ping loss of weight 0.5Œª was used. The training set size of the smartphone and DSLR dataset were 1813 and 3326, respectively. We set Œª = 10.\n",
      "241 7.2. Network architectures\n",
      "242 We provide both PyTorch and Torch implementations. Generator architectures We adopt our architectures from Johnson et al. [23]. We use 6 residual blocks for 128 √ó 128 training images, and 9 residual blocks for 256 √ó 256 or higher-resolution training images. Below, we follow the naming convention used in the Johnson et al.‚Äôs Github repository.\n",
      "243 Let c7s1-k denote a 7 √ó 7 Convolution-InstanceNorm- ReLU layer with k Ô¨Ålters and stride 1. dk denotes a 3 √ó 3 Convolution-InstanceNorm-ReLU layer with k Ô¨Ålters and stride 2. ReÔ¨Çection padding was used to reduce artifacts. Rk denotes a residual block that contains two 3 √ó 3 con- volutional layers with the same number of Ô¨Ålters on both layer. uk denotes a 3 √ó 3 fractional-strided-Convolution- InstanceNorm-ReLU layer with k Ô¨Ålters and stride 1 2 . The network with 6 residual blocks consists of: c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,u128,u64,c7s1-3\n",
      "244 The network with 9 residual blocks consists of: c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,u128 u64,c7s1-3\n",
      "245 Discriminator architectures For discriminator net- works, we use 70 √ó 70 PatchGAN [22]. Let Ck denote a 4 √ó 4 Convolution-InstanceNorm-LeakyReLU layer with k Ô¨Ålters and stride 2. After the last layer, we apply a convo- lution to produce a 1-dimensional output. We do not use InstanceNorm for the Ô¨Årst C64 layer. We use leaky ReLUs with a slope of 0.2. The discriminator architecture is: C64-C128-C256-C512\n",
      "246 7. Appendix\n",
      "247 7.1. Training details\n",
      "248 We train our networks from scratch, with a learning rate of 0.0002. In practice, we divide the objective by 2 while optimizing D, which slows down the rate at which D learns, relative to the rate of G. We keep the same learning rate for the Ô¨Årst 100 epochs and linearly decay the rate to zero over the next 100 epochs. Weights are initialized from a Gaussian distribution N (0, 0.02).\n",
      "249 Cityscapes label‚ÜîPhoto 2975 training images from the Cityscapes training set [4] with image size 128 √ó 128. We used the Cityscapes val set for testing.\n",
      "250 Maps‚Üîaerial photograph 1096 training images were scraped from Google Maps [22] with image size 256 √ó 256. Images were sampled from in and around New York City. Data was then split into train and test about the median lat- itude of the sampling region (with a buffer region added to ensure that no training pixel appeared in the test set).\n",
      "251 Architectural facades labels‚Üîphoto 400 training im-\n",
      "252 ages from the CMP Facade Database [40].\n",
      "253 Edges‚Üíshoes around 50, 000 training images from UT Zappos50K dataset [60]. The model was trained for 5 epochs.\n",
      "254 Horse‚ÜîZebra and Apple‚ÜîOrange We downloaded the images from ImageNet [5] using keywords wild horse, zebra, apple, and navel orange. The images were scaled to 256 √ó 256 pixels. The training set size of each class: 939 (horse), 1177 (zebra), 996 (apple), and 1020 (orange).\n",
      "255 Summer‚ÜîWinter Yosemite The images were down- loaded using Flickr API with the tag yosemite and the date- taken Ô¨Åeld. Black-and-white photos were pruned. The im- ages were scaled to 256 √ó 256 pixels. The training size of each class: 1273 (summer) and 854 ( winter).\n",
      "256 Photo‚ÜîArt for style transfer The art images were downloaded from Wikiart.org. Some artworks that were sketches or too obscene were pruned by hand. The pho- tos were downloaded from Flickr using the combination of tags landscape and landscapephotography. Black-and- white photos were pruned. The images were scaled to 256 √ó 256 pixels. The training set size of each class was 1074 (Monet), 584 (Cezanne), 401 (Van Gogh), 1433 (Ukiyo-e), and 6853 (Photographs). The Monet dataset was particularly pruned to include only landscape paintings, and the Van Gogh dataset included only his later works that rep- resent his most recognizable artistic style.\n",
      "257 Monet‚Äôs paintings‚Üíphotos To achieve high resolution while conserving memory, we used random square crops of the original images for training. To generate results, we passed images of width 512 pixels with correct aspect ra- tio to the generator network as input. The weight for the identity mapping loss was 0.5Œª where Œª was the weight for cycle consistency loss. We set Œª = 10.\n",
      "258 \f",
      "\n"
     ]
    }
   ],
   "source": [
    "translated_pack = []\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    paragraph_line = \" \".join(paragraph.split(\"\\n\"))\n",
    "    if len(paragraph_line) > 0 and paragraph_line[0] != \"5\":\n",
    "#         print (i, paragraph_line)\n",
    "        translation = translator.translate(paragraph_line, src='en', dest='ru')\n",
    "        translated_text = translation.text\n",
    "        translated_pack.append(formating_paragraph_symbols(translated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:08:09.699797Z",
     "start_time": "2020-12-19T15:08:09.675810Z"
    }
   },
   "outputs": [],
   "source": [
    "translated_text = \"\\n\\n\".join(translated_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:08:09.730814Z",
     "start_time": "2020-12-19T15:08:09.707789Z"
    }
   },
   "outputs": [],
   "source": [
    "translated_txt_name = pdf_name[:-4] + \"_translated_symb.txt\"\n",
    "with open(translated_txt_name, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T15:08:09.746766Z",
     "start_time": "2020-12-19T15:08:09.735774Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(pdf_name[:-4] + \"_original.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
